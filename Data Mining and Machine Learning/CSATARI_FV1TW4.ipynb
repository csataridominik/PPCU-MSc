{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dominik Csatári | FV1TW4 | 2023.10.13.\n",
    "# My Kaggle User is: Dominik Csatári | https://www.kaggle.com/dominikcsatri\n",
    "# For the codes, I've created are highly contributed wiht general documentation code\n",
    "# from https://scikit-learn.org  as this is where I have learned, how to use, each\n",
    "# type of estimator, or what hyperparemeters can be tuned, what libraries are recommended to use.\n",
    "\n",
    "#Set random_seed:\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from tabulate import tabulate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this part i read in the files.\n",
    "X = pd.read_csv('pc_X_train.csv')\n",
    "Final_X_test = pd.read_csv('pc_X_test.csv')\n",
    "Y = pd.read_csv('pc_y_train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First drop id attribute, as it is has no use, regarding predicitions.\n",
    "X = X.drop(X.columns[0], axis=1)\n",
    "Final_x_test = Final_X_test.drop(Final_X_test.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split oour data for training and testing our model\n",
    "random_seed = 42\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y['score'], test_size=0.2, random_state=random_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is for writing out our final predicted y to a file for later upload\n",
    "def WriteOutput(data,file_path=\"output.csv\"):\n",
    "    with open(file_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        writer.writerow(['id', 'score'])\n",
    "        \n",
    "        for idx, score in enumerate(data):\n",
    "            writer.writerow([idx, score])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random forest \n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model on the training set\n",
    "rf_model.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4046240530303031"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the mse result for random forest:\n",
    "# (With the feutre reduction, with has not tbeen fine-tuned yet, meaning\n",
    "# it is mainly the same result that we would have got with simple X)\n",
    "mse_varSelected = mean_squared_error( Y_test, y_pred)\n",
    "mse_varSelected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finla prediciton for kaggle upload\n",
    "Final_pred = rf_model.predict(Final_X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "WriteOutput(Final_pred,'output01.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try Entropy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41764873737373737"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ill try Absolute error so outliers less likely to miss\n",
    "rf_model = RandomForestRegressor(n_estimators=150, criterion=\"absolute_error\",random_state=42)\n",
    "\n",
    "# Train the model on the training set\n",
    "rf_model.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "mse_rf = mean_squared_error( Y_test, y_pred)\n",
    "mse_rf\n",
    "#Its a bit worse, with mse it was 0.39"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/feature_selection.html,\n",
    "# This part ha not been fine-tuned yet, no actual change has been made by this block.\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "x_new = sel.fit_transform(X)\n",
    "\n",
    "#For this dataset we get 0.4 error with the first setup of randomforest\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x_new, Y['score'], test_size=0.2, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolved an error with this transofrmation help:\n",
    "# https://stackoverflow.com/questions/71996617/invalid-classes-inferred-from-unique-values-of-y-expected-0-1-2-3-4-5-got\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "Y_train = le.fit_transform(Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "learn = XGBRegressor(n_estimators = 300, learning_rate = 0.001, max_depth = 10)\n",
    "learn.fit(X_train, Y_train)\n",
    "\n",
    "y_pred = learn.predict(X_test)\n",
    "mse_xb = mean_squared_error( Y_test, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5142629966858754"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#n_estimators = 200, learning_rate = 0.01, max_depth = 10, this was the result I have uploaded initially.\n",
    "mse_xb = mean_squared_error( Y_test, y_pred)\n",
    "mse_xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7289235133649757"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XGBRegressor(n_estimators = 300, learning_rate = 0.001, max_depth = 10)\n",
    "mse_xb = mean_squared_error( Y_test, y_pred)\n",
    "mse_xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_y_pred = learn.predict(Final_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "WriteOutput(Final_y_pred,\"out_XBoost.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this part I've applied a GridSearch for HyperParameter Tuning method, Exhaustive GridSearch.\n",
    "# I've searched for the best model in Models, I've saved, given paramteres from a predefined matrix, Models_res.\n",
    "from sklearn import svm\n",
    "\n",
    "reg_numb = 30;\n",
    "eps_values = 20;\n",
    "\n",
    "regularization = np.linspace(0.008,1,reg_numb)\n",
    "epsilon_value = np.linspace(0.1,0.8,eps_values)\n",
    "\n",
    "Models_res = np.zeros((reg_numb, eps_values))\n",
    "Models = []\n",
    "\n",
    "for i in range(len(regularization)):\n",
    "    for j in range(len(epsilon_value)):\n",
    "        regr = svm.SVR(C=regularization[i],epsilon=epsilon_value[j])\n",
    "\n",
    "        Models.append(regr)\n",
    "        regr.fit(X_train, Y_train)\n",
    "        y_pred=regr.predict(X_test)\n",
    "        mse_SVM = mean_squared_error( Y_test, y_pred)\n",
    "        Models_res[i][j] = mse_SVM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 13)\n",
      "0.7240797743811791\n",
      "--------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------\n",
      "0.756711  0.755161  0.753307  0.750511  0.749576  0.752871  0.755288  0.757292  0.759656  0.762463  0.766102  0.769252  0.768278  0.76491   0.761917  0.758878  0.75748   0.756395  0.755889  0.756794\n",
      "0.729317  0.729319  0.729735  0.73042   0.729904  0.730401  0.729089  0.728432  0.72944   0.731585  0.730652  0.730899  0.731349  0.729867  0.72849   0.727532  0.728491  0.730545  0.731337  0.729472\n",
      "0.728552  0.729663  0.730825  0.731752  0.732383  0.731678  0.729715  0.727389  0.726691  0.725186  0.725233  0.725402  0.725194  0.72408   0.724989  0.726112  0.72787   0.730393  0.73075   0.728791\n",
      "0.728893  0.729589  0.730969  0.732099  0.732623  0.732403  0.730158  0.728402  0.726747  0.725443  0.72503   0.724394  0.724524  0.724712  0.725104  0.726159  0.728258  0.730578  0.731097  0.728831\n",
      "0.728746  0.729432  0.730809  0.732124  0.732923  0.733143  0.730914  0.728323  0.727135  0.725577  0.725141  0.724661  0.724873  0.724596  0.725469  0.726201  0.728034  0.730703  0.731123  0.728749\n",
      "0.728359  0.729459  0.730476  0.732057  0.732774  0.733162  0.732444  0.729753  0.727973  0.725778  0.725418  0.725518  0.724987  0.724938  0.725697  0.726257  0.7281    0.730984  0.731019  0.728658\n",
      "0.727836  0.729373  0.730849  0.732318  0.732816  0.732964  0.732328  0.731699  0.729329  0.726379  0.726077  0.726098  0.725718  0.72572   0.725821  0.726458  0.72819   0.73104   0.731305  0.728652\n",
      "0.727788  0.729318  0.731153  0.733043  0.732832  0.733186  0.732528  0.731918  0.730709  0.727501  0.726675  0.726816  0.726662  0.726458  0.726246  0.727066  0.728819  0.731792  0.731281  0.72867\n",
      "0.727725  0.729826  0.731585  0.733025  0.732996  0.73334   0.732523  0.731932  0.732085  0.72912   0.727575  0.727709  0.727528  0.726904  0.727076  0.72781   0.729608  0.732145  0.731511  0.728769\n",
      "0.727989  0.730193  0.732017  0.733109  0.732987  0.733411  0.73341   0.732475  0.732059  0.730334  0.728793  0.728668  0.728004  0.727322  0.72781   0.728656  0.730308  0.732177  0.731559  0.728771\n",
      "0.728329  0.73055   0.73248   0.733131  0.733291  0.733965  0.733378  0.732383  0.731947  0.731243  0.729587  0.729637  0.728604  0.728141  0.728478  0.729375  0.73086   0.732158  0.731702  0.728924\n",
      "0.728827  0.731044  0.732835  0.733379  0.73368   0.734553  0.733886  0.732736  0.732732  0.731587  0.730247  0.730269  0.729168  0.728795  0.729382  0.730083  0.730891  0.732318  0.731737  0.729068\n",
      "0.729151  0.731536  0.73284   0.733691  0.733864  0.73512   0.734056  0.73355   0.732758  0.731579  0.730915  0.731023  0.729827  0.729587  0.730145  0.73019   0.730922  0.73233   0.731733  0.729151\n",
      "0.729666  0.732057  0.732875  0.734179  0.734489  0.735602  0.734309  0.733593  0.732722  0.73172   0.73164   0.731783  0.730484  0.730497  0.730586  0.730271  0.730949  0.732318  0.731774  0.729209\n",
      "0.729912  0.732309  0.733043  0.734619  0.734802  0.736137  0.734486  0.733718  0.732672  0.731927  0.732337  0.732444  0.730915  0.731264  0.73075   0.730317  0.731313  0.732359  0.731892  0.729176\n",
      "0.730095  0.732399  0.733147  0.735146  0.735293  0.736159  0.734865  0.733887  0.733191  0.732754  0.732539  0.733093  0.731737  0.731696  0.730882  0.730366  0.731621  0.732429  0.731923  0.729274\n",
      "0.730453  0.732634  0.733121  0.7358    0.735754  0.73658   0.735644  0.734089  0.73375   0.733067  0.732551  0.733722  0.732512  0.731669  0.731026  0.730661  0.731681  0.732656  0.73198   0.729318\n",
      "0.73076   0.732878  0.733397  0.73604   0.736407  0.737013  0.736838  0.734238  0.734479  0.733176  0.73256   0.734319  0.733216  0.732086  0.731068  0.731328  0.731671  0.73277   0.732019  0.729324\n",
      "0.731183  0.733029  0.734066  0.736277  0.736929  0.737486  0.737642  0.734268  0.734584  0.733245  0.732622  0.734668  0.733438  0.732287  0.73111   0.731583  0.731834  0.732808  0.731989  0.729318\n",
      "0.731392  0.733027  0.734669  0.736467  0.737283  0.737565  0.738065  0.734893  0.73478   0.733466  0.732829  0.734798  0.733412  0.732175  0.731617  0.731598  0.732131  0.732884  0.731931  0.729302\n",
      "0.731475  0.732976  0.735076  0.736715  0.738032  0.737713  0.738633  0.735489  0.73468   0.733523  0.733493  0.734931  0.733633  0.732217  0.732122  0.731896  0.732588  0.733092  0.731941  0.72932\n",
      "0.731702  0.732926  0.735324  0.736712  0.738981  0.738081  0.738965  0.736256  0.734721  0.734014  0.734099  0.735053  0.733655  0.732812  0.732672  0.731987  0.732852  0.733072  0.731923  0.729318\n",
      "0.731846  0.73306   0.735809  0.737024  0.740043  0.73864   0.739542  0.736888  0.734768  0.73448   0.734241  0.73517   0.733726  0.733579  0.73289   0.732024  0.733155  0.733058  0.731946  0.729328\n",
      "0.731944  0.733103  0.736194  0.73727   0.740939  0.739211  0.7398    0.737781  0.734748  0.735176  0.734232  0.7352    0.733748  0.734388  0.732923  0.732036  0.733317  0.733027  0.731941  0.729317\n",
      "0.732107  0.733158  0.736522  0.737643  0.741459  0.739704  0.740261  0.738196  0.734863  0.735891  0.734266  0.735387  0.733953  0.734848  0.732992  0.732059  0.733436  0.733091  0.732025  0.729313\n",
      "0.732203  0.733142  0.736634  0.737826  0.741831  0.74019   0.740675  0.738877  0.735317  0.736082  0.734438  0.735753  0.734694  0.734876  0.733074  0.732045  0.733414  0.733097  0.732007  0.729296\n",
      "0.732299  0.733144  0.736921  0.738282  0.742277  0.741221  0.740934  0.739719  0.735717  0.736139  0.734522  0.735816  0.735062  0.734947  0.733141  0.732256  0.73361   0.733178  0.732022  0.729299\n",
      "0.732639  0.733161  0.73726   0.738567  0.74279   0.741825  0.74143   0.739593  0.736269  0.736156  0.734746  0.73591   0.735256  0.735149  0.733241  0.732399  0.733663  0.73318   0.73204   0.729281\n",
      "0.732659  0.733221  0.737631  0.738929  0.743729  0.742276  0.741697  0.739619  0.737023  0.736242  0.734785  0.73593   0.735275  0.735168  0.733242  0.732382  0.73376   0.733171  0.732043  0.729244\n",
      "0.73268   0.733215  0.737962  0.739135  0.744166  0.742541  0.741888  0.739492  0.737639  0.736249  0.734832  0.736049  0.735297  0.735231  0.733244  0.732414  0.733688  0.73328   0.731996  0.729266\n",
      "--------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------\n"
     ]
    }
   ],
   "source": [
    "# ChatGBT help\n",
    "min_index = np.unravel_index(np.argmin(Models_res, axis=None), Models_res.shape)\n",
    "print(min_index)\n",
    "print(Models_res[min_index])\n",
    "print(tabulate(Models_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (x,y) index to linear index:\n",
    "ind = (min_index[0]-1)*reg_numb+min_index[1]\n",
    "Best_SVM_Model = Models[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_SVM_Pred = Best_SVM_Model.predict(Final_x_test)\n",
    "WriteOutput(Final_SVM_Pred,\"SVM_Output.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8673441517785204\n"
     ]
    }
   ],
   "source": [
    "# For KNN, It was hard to define a good initial estimate of the correct parameters, so I've turned to other methods.\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "neigh = KNeighborsRegressor(n_neighbors=70,weights='distance')\n",
    "neigh.fit(X_train, Y_train)\n",
    "y_pred = neigh.predict(X_test)\n",
    "mse_KNN = mean_squared_error( Y_test, y_pred)\n",
    "rmse_KNN = np.sqrt(mse_KNN)\n",
    "\n",
    "print(rmse_KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_KNN_Pred = neigh.predict(Final_x_test)\n",
    "WriteOutput(Final_KNN_Pred,\"KNN_Output.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6480723623018834\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this part I've applied a GridSearch for HyperParameter Tuning method, Exhaustive GridSearch.\n",
    "# I've searched for the best model in Models, I've saved, given paramteres from a predefined matrix, Models_res.\n",
    "lr_numb = 30;\n",
    "numb_trees = 10;\n",
    "\n",
    "learning_rate = np.linspace(0.001,1,lr_numb)\n",
    "trees = np.linspace(70,120,numb_trees)\n",
    "\n",
    "Models_res = np.zeros((numb_trees, lr_numb))\n",
    "Models = []\n",
    "\n",
    "for i in range(len(trees)):\n",
    "    for j in range(len(learning_rate)):\n",
    "                \n",
    "        AdaBoost = AdaBoostRegressor(random_state=42, n_estimators=int(trees[i]), learning_rate = learning_rate[j])\n",
    "        Models.append(AdaBoost)\n",
    "\n",
    "        AdaBoost.fit(X_train, Y_train)\n",
    "\n",
    "        y_pred = AdaBoost.predict(X_test)\n",
    "        mse_ada = mean_squared_error( Y_test, y_pred)\n",
    "        rmse_ada = np.sqrt(mse_ada)\n",
    "\n",
    "        Models_res[i][j] = rmse_ada\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 15)\n",
      "0.6447542631134845\n",
      "--------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------\n",
      "0.678024  0.667399  0.665222  0.659798  0.656282  0.655729  0.653166  0.657668  0.657633  0.659108  0.654097  0.658257  0.651994  0.662349  0.659229  0.649582  0.659386  0.659301  0.655524  0.661537  0.654457  0.648763  0.66222   0.65282   0.654428  0.660382  0.656543  0.652657  0.662783  0.659899\n",
      "0.67635   0.6665    0.664084  0.657608  0.655046  0.653636  0.652331  0.658394  0.653441  0.658499  0.653998  0.656333  0.648936  0.65962   0.658396  0.647992  0.658785  0.660215  0.650101  0.659392  0.653252  0.650522  0.657062  0.651235  0.655224  0.659434  0.654062  0.64746   0.665761  0.654189\n",
      "0.676989  0.665987  0.662897  0.658089  0.655068  0.653939  0.653206  0.65859   0.654038  0.661021  0.654256  0.659037  0.648935  0.657257  0.659256  0.648394  0.654657  0.659679  0.650698  0.656176  0.651934  0.649144  0.657539  0.650498  0.656458  0.660255  0.650739  0.652238  0.659351  0.654222\n",
      "0.675727  0.665472  0.66296   0.658484  0.654531  0.651914  0.654458  0.658957  0.650879  0.658661  0.65261   0.661332  0.64657   0.656893  0.660405  0.644828  0.654138  0.657945  0.651187  0.659491  0.651168  0.652884  0.657381  0.650858  0.660913  0.656121  0.652583  0.65156   0.660092  0.651204\n",
      "0.676079  0.66429   0.663192  0.658225  0.653788  0.653611  0.654185  0.658818  0.653618  0.659897  0.653402  0.658613  0.645855  0.657392  0.654546  0.64598   0.655507  0.658651  0.648137  0.658791  0.649847  0.649529  0.654983  0.654038  0.657382  0.655173  0.651743  0.653366  0.662969  0.649165\n",
      "0.676275  0.663824  0.662564  0.658062  0.655786  0.655451  0.655747  0.658818  0.656714  0.660406  0.652909  0.659896  0.647901  0.656573  0.656511  0.645854  0.655167  0.660351  0.64999   0.65758   0.650404  0.650841  0.653647  0.655197  0.657974  0.655323  0.647418  0.652985  0.659127  0.649394\n",
      "0.676567  0.664144  0.661096  0.655672  0.655655  0.654272  0.655806  0.66024   0.658826  0.659855  0.654119  0.655133  0.646335  0.658775  0.655815  0.645776  0.657467  0.657461  0.651049  0.658152  0.64928   0.65206   0.653704  0.654228  0.655142  0.65641   0.652512  0.651825  0.658326  0.650864\n",
      "0.676519  0.663964  0.66094   0.655022  0.655704  0.655588  0.653615  0.658013  0.657515  0.657986  0.654615  0.655819  0.648217  0.65991   0.6554    0.644754  0.657071  0.658479  0.649589  0.659047  0.646617  0.654491  0.65243   0.654342  0.656377  0.65393   0.649638  0.652709  0.658221  0.652576\n",
      "0.677215  0.662029  0.661882  0.655773  0.654735  0.65368   0.654147  0.659451  0.655851  0.657913  0.656057  0.652549  0.646753  0.659047  0.652728  0.645983  0.65558   0.660083  0.650335  0.657993  0.649472  0.655471  0.649876  0.654534  0.653841  0.656591  0.651782  0.652739  0.658078  0.653241\n",
      "0.677726  0.661482  0.660614  0.657745  0.653791  0.654846  0.654998  0.657938  0.65805   0.658488  0.654528  0.654509  0.643974  0.660207  0         0         0         0         0         0         0         0         0         0         0         0         0         0         0         0\n",
      "--------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------\n",
      "AdaBoostRegressor(learning_rate=0.31103448275862067, n_estimators=81,\n",
      "                  random_state=42)\n"
     ]
    }
   ],
   "source": [
    "# For min_index we get the right value for the best estiamter.\n",
    "# (I had stop the runing, hence the 0's, but when searching for the best estimator, I've disregarded them. )\n",
    "min_index = np.unravel_index(np.argmin(Models_res, axis=None), Models_res.shape)\n",
    "\n",
    "print(min_index)\n",
    "print(Models_res[min_index])\n",
    "print(tabulate(Models_res))\n",
    "ind = (min_index[0]-1)*lr_numb+min_index[1]\n",
    "\n",
    "Best_AdaBoost_Model = Models[ind]\n",
    "print(Models[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_y = Best_AdaBoost_Model.predict(Final_x_test)\n",
    "\n",
    "WriteOutput(Final_y,\"AdaBoost_output.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForest Regressor with Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"max_depth\": [10, None], \"min_samples_split\": [5, 15]}\n",
    "\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "rf_model.fit(X_train, Y_train)\n",
    "\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "search = HalvingGridSearchCV(rf_model, param_grid, resource='n_estimators',\n",
    "                             max_resources=120,\n",
    "                              random_state=0).fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': None, 'min_samples_split': 15, 'n_estimators': 120}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6289397996817372"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This gives us a values which is below the one, we have managed to gather with simple trial-and-error.\n",
    "# The search cut of too early.\n",
    "y_pred= search.best_estimator_.predict(X_test)\n",
    "np.sqrt(mean_squared_error(y_pred,Y_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
