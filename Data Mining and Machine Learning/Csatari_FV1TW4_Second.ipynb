{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dominik Csatári | FV1TW4 | 2024.01.09.\n",
    "# My Kaggle User is: Dominik Csatári | https://www.kaggle.com/dominikcsatri\n",
    "# For the codes, I've created are highly contributed wiht general documentation code\n",
    "# from https://scikit-learn.org  as this is where I have learned, how to use, each\n",
    "# type of estimator, or what hyperparemeters can be tuned, what libraries are recommended to use.\n",
    "\n",
    "#Set random_seed:\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from tabulate import tabulate\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this part i read in the files.\n",
    "X = pd.read_csv('pc_X_train.csv')\n",
    "Final_X_test = pd.read_csv('pc_X_test.csv')\n",
    "Y = pd.read_csv('pc_y_train.csv')\n",
    "X = X.drop(X.columns[0], axis=1)\n",
    "Final_x_test = Final_X_test.drop(Final_X_test.columns[0], axis=1)\n",
    "\n",
    "# Split oour data for training and testing our model\n",
    "random_seed = 42\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y['score'], test_size=0.2, random_state=random_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This figure represent the distribution of y values in the test and train set and Y set.\n",
    "plt.figure()\n",
    "Y['score'].hist(label='Y')\n",
    "Y_train.hist(label='Y_train')\n",
    "Y_test.hist(label='Y_test')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is for writing out our final predicted y to a file for later upload\n",
    "def WriteOutput(data,file_path=\"output.csv\"):\n",
    "    with open(file_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        writer.writerow(['id', 'score'])\n",
    "        \n",
    "        for idx, score in enumerate(data):\n",
    "            writer.writerow([idx, score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_down_time              False\n",
      "std_down_time               False\n",
      "max_down_time               False\n",
      "sum_down_time               False\n",
      "mean_up_time                False\n",
      "                            ...  \n",
      "change_14_mean              False\n",
      "change_14_std               False\n",
      "count_action_time_grade2    False\n",
      "max_cursor_grade2           False\n",
      "max_word_count_grade2       False\n",
      "Length: 468, dtype: bool\n",
      "mean_down_time              False\n",
      "std_down_time               False\n",
      "max_down_time               False\n",
      "sum_down_time               False\n",
      "mean_up_time                False\n",
      "                            ...  \n",
      "change_14_mean              False\n",
      "change_14_std               False\n",
      "count_action_time_grade2    False\n",
      "max_cursor_grade2           False\n",
      "max_word_count_grade2       False\n",
      "Length: 468, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "# This is where I check if there is any missing values, there were none.\n",
    "print(np.isnan(X_train).any()==True)\n",
    "print(np.isinf(X_train).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this part I create the PCA transformation, with the scaled set.\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "prec = 0.3\n",
    "n_components = int(np.floor(X.shape[1]*prec))\n",
    "\n",
    "pca = PCA(n_components='mle',svd_solver='full')\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "# Use the transformed data for further analysis or modeling\n",
    "X_test_pca = pca.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "468\n",
      "PCA mle coded:\n",
      "349\n"
     ]
    }
   ],
   "source": [
    "# This is where I compare it with the original, to see how many dimensions could we decrease.\n",
    "print(\"Original:\")\n",
    "print(X.shape[1])\n",
    "print(\"PCA mle coded:\")\n",
    "print(X_train_pca.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have tried with various amount of columns, with trial and error to see coharance. With 49 column it still gave a great result, with knn and random forest as well, rmse being below 0.7 . treshold to variance is at 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1580, 49)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thi is a randomforest model fitted on the pca set\n",
    "rf_model = RandomForestRegressor(n_estimators=80, random_state=42)\n",
    "\n",
    "# Train the model on the training set\n",
    "rf_model.fit(X_train_pca, Y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = rf_model.predict(X_test_pca)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6797452046502053"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mse_varSelected = mean_squared_error( Y_test, y_pred, squared=False)\n",
    "mse_varSelected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining Features: (1580, 229)\n",
      "Dropped columns: [229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348]\n"
     ]
    }
   ],
   "source": [
    "# This code part is written by me assisted with ChatGBT\n",
    "# there is sklearn libary for it, which I already used,\n",
    "# but for me it was easier to work with.\n",
    "import numpy as np\n",
    "#0 0.001, 0.01, 0.1, 1, 10\n",
    "def drop_low_variance_columns(X_train_pca, threshold=0.001):\n",
    "    Dropped = []\n",
    "    \n",
    "    # Calculate the variance for each column\n",
    "    variances = np.var(X_train_pca, axis=0)\n",
    "    \n",
    "    # Identify columns with variance below the threshold\n",
    "    low_variance_columns = np.where(variances < threshold)[0]\n",
    "    \n",
    "    # Drop low variance columns and store the column numbers in 'Dropped'\n",
    "    X_train_pca = np.delete(X_train_pca, low_variance_columns, axis=1)\n",
    "    Dropped.extend(low_variance_columns)\n",
    "\n",
    "    print(\"Remaining Features:\", X_train_pca.shape)\n",
    "    return X_train_pca, Dropped\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train_pca, Dropped = drop_low_variance_columns(X_train_pca)\n",
    "\n",
    "print(\"Dropped columns:\", Dropped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you delete feature from the test set as well\n",
    "X_test_pca = np.delete(X_test_pca, Dropped, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part I create a Final_X prediciton for kaggle upload, with the dataset\n",
    "# having reduced its dimension with pca and by variance hreshold as well.\n",
    "\n",
    "X_final_scaled = scaler.fit_transform(Final_x_test)\n",
    "X_Final = pca.transform(X_final_scaled)\n",
    "asd = np.delete(X_Final, Dropped, axis=1)\n",
    "\n",
    "print(X_Final.all() == asd.all())\n",
    "Final_pred = rf_model.predict(asd)\n",
    "WriteOutput(Final_pred,\"RandomForest_PCA3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6795972559101549\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "neigh = KNeighborsRegressor(n_neighbors=50,algorithm='auto',weights='distance')\n",
    "neigh.fit(X_train_pca, Y_train)\n",
    "y_pred = neigh.predict(X_test_pca)\n",
    "mse_KNN = mean_squared_error( Y_test, y_pred)\n",
    "rmse_KNN = np.sqrt(mse_KNN)\n",
    "\n",
    "print(rmse_KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM for dropped columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I do the grid search for SVR as well with having one mor dimension, the variance.\n",
    "\n",
    "reg_numb = 30;\n",
    "eps_values = 20;\n",
    "\n",
    "regularization = np.linspace(0.008,1,reg_numb)\n",
    "epsilon_value = np.linspace(0.1,0.8,eps_values)\n",
    "\n",
    "Models_res = np.zeros((reg_numb, eps_values))\n",
    "Models = []\n",
    "\n",
    "for i in range(len(regularization)):\n",
    "    for j in range(len(epsilon_value)):\n",
    "        regr = svm.SVR(C=regularization[i],epsilon=epsilon_value[j])\n",
    "\n",
    "        Models.append(regr)\n",
    "        regr.fit(X_train_pca, Y_train)\n",
    "        y_pred=regr.predict(X_test_pca)\n",
    "        mse_SVM = mean_squared_error( Y_test, y_pred)\n",
    "        Models_res[i][j] = mse_SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 11)\n",
      "0.4322714367724334\n",
      "--------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------\n",
      "0.612624  0.613848  0.614714  0.613532  0.613543  0.615625  0.617952  0.622826  0.626826  0.63244   0.63781   0.641899  0.643089  0.645203  0.644193  0.648014  0.652638  0.653344  0.66223   0.668075\n",
      "0.483872  0.484691  0.488385  0.487467  0.487482  0.489012  0.490012  0.490583  0.491191  0.489895  0.489807  0.48911   0.491856  0.491929  0.491015  0.492137  0.494295  0.494145  0.496232  0.499953\n",
      "0.463054  0.463957  0.466261  0.46517   0.465095  0.465019  0.465136  0.466843  0.465715  0.465857  0.466826  0.465762  0.465948  0.466137  0.467688  0.469614  0.471448  0.47231   0.472715  0.473739\n",
      "0.454247  0.45579   0.455174  0.452419  0.452454  0.450711  0.452289  0.455493  0.453675  0.454855  0.454846  0.45352   0.455771  0.456363  0.456362  0.45906   0.460441  0.460039  0.460285  0.460874\n",
      "0.449052  0.449543  0.449541  0.446929  0.444823  0.444949  0.448401  0.448269  0.449153  0.448745  0.448371  0.447418  0.448921  0.4502    0.44934   0.451502  0.4536    0.452454  0.453509  0.456958\n",
      "0.445403  0.444587  0.445127  0.442875  0.441112  0.442532  0.445382  0.446517  0.445225  0.443473  0.442317  0.44397   0.445104  0.445618  0.444605  0.446811  0.448157  0.449259  0.451176  0.453419\n",
      "0.441805  0.441749  0.441629  0.439255  0.439592  0.441853  0.443409  0.44363   0.442683  0.440094  0.438939  0.441271  0.441237  0.442435  0.441904  0.442375  0.444128  0.446449  0.449367  0.451715\n",
      "0.438245  0.440023  0.439332  0.437317  0.438947  0.441295  0.441148  0.442009  0.440178  0.438693  0.438299  0.43788   0.438842  0.439803  0.43934   0.440488  0.442056  0.446112  0.448482  0.450552\n",
      "0.436237  0.436855  0.437307  0.436605  0.438461  0.440853  0.439891  0.440303  0.439147  0.437884  0.436748  0.435649  0.437382  0.438525  0.438289  0.439628  0.44089   0.445685  0.447041  0.449597\n",
      "0.435199  0.435514  0.436333  0.43662   0.438285  0.440012  0.438885  0.438687  0.438041  0.436867  0.435412  0.43417   0.435898  0.437578  0.438501  0.438516  0.439862  0.445205  0.446347  0.448394\n",
      "0.434696  0.43479   0.436308  0.437138  0.438253  0.439893  0.438063  0.436128  0.437166  0.43573   0.434119  0.433287  0.434853  0.437057  0.438437  0.437436  0.439236  0.444581  0.445445  0.447427\n",
      "0.434613  0.434575  0.436448  0.436949  0.438107  0.43943   0.437092  0.434816  0.43595   0.43536   0.433393  0.432763  0.434305  0.437141  0.438099  0.437014  0.438527  0.443654  0.444703  0.446488\n",
      "0.434508  0.434467  0.436618  0.436651  0.438411  0.438278  0.436646  0.434629  0.43575   0.435358  0.433413  0.432421  0.434123  0.437003  0.437258  0.436119  0.43855   0.442948  0.444167  0.445709\n",
      "0.434465  0.434609  0.436843  0.436828  0.438042  0.437584  0.436706  0.434765  0.435474  0.434999  0.433575  0.432271  0.434173  0.43693   0.436577  0.435867  0.438796  0.442096  0.443572  0.445158\n",
      "0.43444   0.434749  0.43684   0.437189  0.437845  0.43759   0.437049  0.434927  0.435247  0.434602  0.433896  0.432347  0.434254  0.436782  0.436537  0.435943  0.438728  0.441277  0.443153  0.444717\n",
      "0.434737  0.434982  0.436907  0.437631  0.438139  0.437911  0.43699   0.434882  0.434636  0.43434   0.434056  0.432732  0.434436  0.437045  0.436585  0.436191  0.438603  0.440759  0.442994  0.444189\n",
      "0.43479   0.435498  0.437078  0.438489  0.438373  0.438372  0.437254  0.434391  0.434408  0.433948  0.434319  0.433174  0.434707  0.437293  0.436367  0.436543  0.438313  0.440519  0.442854  0.443755\n",
      "0.435029  0.43594   0.437559  0.439279  0.438621  0.438695  0.437345  0.434026  0.434144  0.433979  0.434399  0.433591  0.435025  0.437301  0.436208  0.436751  0.438021  0.440435  0.442796  0.443248\n",
      "0.435738  0.436681  0.437925  0.43939   0.438493  0.438589  0.437298  0.434282  0.433948  0.433892  0.434465  0.433911  0.435246  0.437185  0.436116  0.436597  0.438008  0.440265  0.442443  0.442484\n",
      "0.436284  0.437418  0.438075  0.439285  0.438501  0.438496  0.43722   0.434599  0.433854  0.433788  0.434467  0.434181  0.43533   0.436965  0.436168  0.436459  0.438111  0.44014   0.441937  0.441819\n",
      "0.436975  0.437905  0.438104  0.43894   0.438397  0.438584  0.437208  0.434904  0.433782  0.433717  0.434423  0.434579  0.435304  0.436728  0.436332  0.436508  0.438152  0.440101  0.441463  0.441279\n",
      "0.437613  0.438273  0.43805   0.438377  0.43821   0.438651  0.437174  0.435082  0.433699  0.43373   0.434566  0.435115  0.435321  0.436627  0.436525  0.436706  0.437999  0.440091  0.44099   0.441151\n",
      "0.437999  0.438482  0.437885  0.438063  0.43833   0.43884   0.43732   0.43534   0.433574  0.433608  0.434934  0.435619  0.43539   0.436532  0.436716  0.437108  0.437836  0.439913  0.440872  0.441273\n",
      "0.438473  0.438208  0.437528  0.437955  0.438576  0.438906  0.437582  0.435523  0.433406  0.433483  0.435168  0.436045  0.435573  0.436461  0.436798  0.437331  0.437739  0.439684  0.440784  0.441513\n",
      "0.438732  0.43822   0.437256  0.438013  0.438806  0.438985  0.438035  0.435811  0.433248  0.433561  0.435496  0.436367  0.435657  0.436242  0.436799  0.437479  0.437702  0.439585  0.440676  0.441796\n",
      "0.438813  0.437959  0.436871  0.438161  0.439222  0.439184  0.438475  0.43606   0.433331  0.433652  0.435631  0.436639  0.435706  0.435988  0.436709  0.437584  0.4378    0.439478  0.440606  0.442086\n",
      "0.438973  0.437641  0.436674  0.438401  0.439607  0.439464  0.438748  0.436342  0.433538  0.433829  0.435805  0.43676   0.435741  0.435748  0.436532  0.437633  0.437941  0.439359  0.440374  0.442387\n",
      "0.439106  0.437572  0.436572  0.438621  0.440045  0.439622  0.439068  0.436565  0.43377   0.433979  0.435999  0.436765  0.435708  0.435544  0.436442  0.43755   0.43792   0.439315  0.440232  0.442677\n",
      "0.439291  0.437678  0.436731  0.438861  0.440423  0.439978  0.439401  0.436805  0.434034  0.43417   0.436236  0.436896  0.435707  0.435364  0.436409  0.437364  0.437937  0.439269  0.440102  0.442937\n",
      "0.439363  0.437725  0.436852  0.439144  0.440601  0.440379  0.439808  0.437034  0.434379  0.434477  0.436538  0.436908  0.435633  0.435161  0.436407  0.437271  0.437872  0.439206  0.440096  0.443073\n",
      "--------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------  --------\n"
     ]
    }
   ],
   "source": [
    "min_index = np.unravel_index(np.argmin(Models_res, axis=None), Models_res.shape)\n",
    "print(min_index)\n",
    "print(Models_res[min_index])\n",
    "print(tabulate(Models_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4322714367724334\n"
     ]
    }
   ],
   "source": [
    "# Estimating with the best model I found with grid search\n",
    "ind = (min_index[0]-1)*reg_numb+min_index[1]\n",
    "Best_SVM_Model = Models[ind]\n",
    "print(Models_res[min_index[0]][min_index[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try for different variances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining Features: (1580, 349)\n",
      "Remaining Features: (1580, 349)\n",
      "Remaining Features: (1580, 229)\n",
      "Remaining Features: (1580, 177)\n",
      "Remaining Features: (1580, 103)\n",
      "Remaining Features: (1580, 49)\n",
      "Remaining Features: (1580, 8)\n",
      "Remaining Features: (1580, 5)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reg_numb = 30;\n",
    "eps_values = 20;\n",
    "\n",
    "# Hyper parameters to be tuned:\n",
    "regularization = np.linspace(0.008,1,reg_numb)\n",
    "epsilon_value = np.linspace(0.1,0.8,eps_values)\n",
    "thresholds = [0, 0.001, 0.01, 0.1, 1, 10, 20]\n",
    "\n",
    "\n",
    "Models_res = np.zeros((len(thresholds),reg_numb, eps_values))\n",
    "Models = []\n",
    "\n",
    "X_train_pca, Dropped = drop_low_variance_columns(X_train_pca)\n",
    "X_test_pca = np.delete(X_test_pca, Dropped, axis=1)\n",
    "\n",
    "for z in range(len(thresholds)):\n",
    "\n",
    "    X_train_pca, Dropped = drop_low_variance_columns(X_train_pca,threshold=thresholds[z])\n",
    "    X_test_pca = np.delete(X_test_pca, Dropped, axis=1)\n",
    "\n",
    "    for i in range(len(regularization)):\n",
    "        for j in range(len(epsilon_value)):\n",
    "            regr = svm.SVR(C=regularization[i],epsilon=epsilon_value[j])\n",
    "\n",
    "            Models.append(regr)\n",
    "            regr.fit(X_train_pca, Y_train)\n",
    "            y_pred=regr.predict(X_test_pca)\n",
    "            mse_SVM = mean_squared_error( Y_test, y_pred)\n",
    "            Models_res[z][i][j] = mse_SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 29, 7)\n",
      "0.4114847111925296\n",
      "------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------\n",
      "[0.64241617 0.64052298 0.63847862 0.64041359 0.64228693 0.64661634  [0.48952212 0.4881898  0.48988237 0.49249192 0.49162112 0.49273856  [0.46390363 0.46519986 0.46724226 0.46728792 0.46589417 0.46517743  [0.45280932 0.45463651 0.45282495 0.45245312 0.45094999 0.45054587  [0.44584854 0.44586609 0.44390814 0.44296998 0.4422663  0.44187863  [0.43900272 0.43955925 0.43788916 0.4377046  0.43746501 0.43813234  [0.43514791 0.43599594 0.43369755 0.43308085 0.43458384 0.43518231  [0.43292911 0.43309091 0.43019272 0.42963484 0.4316557  0.43232324  [0.43101663 0.42994    0.4269065  0.42712106 0.42933577 0.43024972  [0.42914462 0.42740431 0.42497906 0.42494841 0.42702277 0.42837324  [0.42790442 0.42554796 0.42305593 0.42319642 0.42497638 0.42645239  [0.42696976 0.42434536 0.42156544 0.42180583 0.42317557 0.42435949  [0.42641198 0.42314435 0.42068301 0.42064467 0.4212557  0.42240912  [0.42546248 0.4225889  0.42037709 0.41976415 0.41931617 0.42060207  [0.42458301 0.42256521 0.42006242 0.41886846 0.4179106  0.41935026  [0.424192   0.42251233 0.42010902 0.41833685 0.41750219 0.41834784  [0.42364938 0.42243018 0.42007845 0.41790579 0.4173101  0.41739801  [0.42318248 0.42199142 0.42007048 0.41769208 0.4168623  0.41678184  [0.42283463 0.42195613 0.42014948 0.41728817 0.41641355 0.41635085  [0.42247154 0.42210326 0.42024154 0.41700637 0.41590405 0.4159723   [0.42229138 0.42236996 0.4203552  0.41676994 0.41565435 0.41558972  [0.4223162  0.42259375 0.42041936 0.41675381 0.41536463 0.41526331  [0.42247255 0.42282436 0.42052628 0.41684373 0.41501759 0.41491487  [0.42253668 0.4230142  0.42075618 0.41670849 0.41477696 0.41455345  [0.42258184 0.42310253 0.42086304 0.41656279 0.41457641 0.41433129  [0.42280723 0.4230141  0.42084196 0.41635317 0.41438054 0.41420927  [0.42298857 0.42293527 0.42069013 0.41596485 0.41428226 0.41409809  [0.42301514 0.42293199 0.42046334 0.41563536 0.4142198  0.41395046  [0.42307018 0.42288227 0.42024181 0.4154915  0.41417088 0.41382826  [0.4231727  0.42274308 0.42010651 0.41533193 0.41411663 0.41374175\n",
      " 0.6507391  0.65460083 0.65902568 0.66493693 0.66945443 0.67385935   0.49714748 0.49621377 0.49537645 0.49373491 0.49584386 0.49566433   0.4655392  0.46721276 0.466958   0.46748    0.46893697 0.46804378   0.45099322 0.45293071 0.45342751 0.45399831 0.45562162 0.45623338   0.44382262 0.44442677 0.44384174 0.44500675 0.44621454 0.45027133   0.43869834 0.43731037 0.43634368 0.4391019  0.44132571 0.44349168   0.43375193 0.43113989 0.4323497  0.43363599 0.4362072  0.43833749   0.43029204 0.42774678 0.42804366 0.42994386 0.43283029 0.43413704   0.4281399  0.42478203 0.42553891 0.42738335 0.4292224  0.43044394   0.42569614 0.42244298 0.42370387 0.42541086 0.42536331 0.42745436   0.42329429 0.42092055 0.42176767 0.42339612 0.42336181 0.42489969   0.42151228 0.41983393 0.4201905  0.42183158 0.42178788 0.42344861   0.42016332 0.4186544  0.41874863 0.42053048 0.42064265 0.42210524   0.4191929  0.41764792 0.41786106 0.41951262 0.41993989 0.42099647   0.41831628 0.41705111 0.41700118 0.41871324 0.41916866 0.42024799   0.41758795 0.41635387 0.41635775 0.41809633 0.41842166 0.41973546   0.4167134  0.41578274 0.41585597 0.41758602 0.41794598 0.41956311   0.41614861 0.41530957 0.41548408 0.41712948 0.4174563  0.4192905    0.41580578 0.41481853 0.41514281 0.4166951  0.41690899 0.41870575   0.41533489 0.41441438 0.41502042 0.41630936 0.41631672 0.41798967   0.41492998 0.41420039 0.41477873 0.41588482 0.41570294 0.41727771   0.41435603 0.41387185 0.4144392  0.41538831 0.41516056 0.41684159   0.41376532 0.41350513 0.4140631  0.41486243 0.41474667 0.41645742   0.41329346 0.41316302 0.41367275 0.41437919 0.41440462 0.41627241   0.41282527 0.41275107 0.4133677  0.4139586  0.41411851 0.41621377   0.41243318 0.41238004 0.41304673 0.41352076 0.41394942 0.41627357   0.41209112 0.41198077 0.41270174 0.41316333 0.41391039 0.41626932   0.41186554 0.41167376 0.41238674 0.41295723 0.4140209  0.41635368   0.41170603 0.41153044 0.41215453 0.4128913  0.41415204 0.41652941   0.41155528 0.4114934  0.41195416 0.4129195  0.4143181  0.41659131\n",
      " 0.67760733 0.68102462 0.68565957 0.6870566  0.6878519  0.69066074   0.49694402 0.49825102 0.50116024 0.5022189  0.50579448 0.50413303   0.4680371  0.46950464 0.47065243 0.4737008  0.47416398 0.47678146   0.45807387 0.45683077 0.45759355 0.45987118 0.461158   0.46038303   0.45097536 0.44994272 0.45042646 0.45087091 0.44975457 0.45157669   0.44452625 0.44420589 0.44348999 0.44377095 0.44338707 0.44545306   0.43882613 0.43870689 0.43823484 0.43806468 0.4388905  0.44030997   0.43435309 0.43462138 0.43455363 0.4337526  0.43511064 0.43732911   0.43097414 0.43166843 0.43156163 0.4308723  0.43341382 0.43569102   0.42832873 0.42886024 0.42924731 0.42961838 0.43211131 0.43447744   0.42641772 0.42661207 0.42763016 0.428885   0.43104699 0.43370363   0.42452379 0.42525162 0.42667039 0.42835877 0.43034273 0.43262244   0.42318343 0.42450605 0.42579005 0.42741756 0.42957947 0.43181037   0.42234959 0.42377089 0.42471975 0.42647992 0.42902477 0.43111115   0.42169992 0.42307469 0.42398586 0.42586092 0.42876767 0.43059987   0.42127014 0.42245591 0.42340647 0.42546911 0.42820247 0.43011309   0.4208689  0.42178373 0.42284029 0.4248712  0.42769922 0.42956951   0.42022817 0.42119082 0.42237976 0.4243805  0.42726392 0.42918972   0.41976279 0.42068014 0.42191175 0.42415577 0.4267772  0.42887797   0.41930343 0.42019041 0.42170319 0.42387957 0.42616887 0.42878291   0.41886517 0.41982303 0.42147777 0.42346265 0.42565574 0.42877455   0.41837908 0.41953539 0.42127836 0.42312774 0.42557977 0.42862321   0.4179425  0.41928314 0.42103602 0.42305979 0.42554258 0.42838455   0.41753378 0.41905511 0.42084181 0.42297912 0.42539061 0.4281667    0.4172274  0.41893637 0.42069582 0.42292254 0.4252033  0.42799926   0.4170314  0.41893219 0.42063477 0.42271954 0.42512301 0.42780533   0.41697932 0.41896914 0.4204935  0.42252673 0.42503472 0.42767199   0.41695618 0.41904025 0.42043781 0.42240596 0.42491558 0.42755224   0.41705654 0.41904247 0.42046581 0.42238194 0.42493638 0.4275423    0.41724875 0.41901302 0.4205887  0.42235043 0.42506207 0.42753173\n",
      " 0.69500867 0.70192893]                                              0.50689669 0.51045932]                                              0.47708132 0.47858275]                                              0.46100447 0.46324752]                                              0.45229886 0.4534032 ]                                              0.44730675 0.44744122]                                              0.44244019 0.44458516]                                              0.43986994 0.44267675]                                              0.43804726 0.44139658]                                              0.43672869 0.44064632]                                              0.4356694  0.43955273]                                              0.43488171 0.4388058 ]                                              0.4345727  0.4380363 ]                                              0.43444257 0.43735605]                                              0.43392319 0.43719096]                                              0.43341717 0.43707512]                                              0.43305286 0.43690958]                                              0.43266497 0.43677353]                                              0.43234502 0.43653709]                                              0.43206584 0.43620262]                                              0.43173517 0.43605004]                                              0.43140922 0.43600269]                                              0.43112055 0.43593539]                                              0.43105866 0.43584728]                                              0.43109468 0.43579267]                                              0.43113246 0.4358304 ]                                              0.43116651 0.43590257]                                              0.43120614 0.43603676]                                              0.4313084  0.43630507]                                              0.43145343 0.43648977]\n",
      "[0.64239934 0.64052068 0.6384636  0.64039861 0.64226443 0.64659981  [0.4894856  0.48813639 0.48984955 0.49246562 0.4915967  0.49274153  [0.46390751 0.46517842 0.46721004 0.46727939 0.46590013 0.46519337  [0.45279736 0.45462213 0.4528434  0.45243703 0.45097596 0.45053888  [0.44582933 0.44588336 0.44389647 0.44298821 0.44225585 0.44187462  [0.43898769 0.43957121 0.43788869 0.43768458 0.43746987 0.43810772  [0.43515309 0.43598592 0.43371722 0.43308382 0.43458314 0.43517125  [0.43290842 0.43309343 0.43018166 0.42965414 0.4316762  0.43230723  [0.43104657 0.42991517 0.42691454 0.42712974 0.42935073 0.43023523  [0.42915265 0.4273939  0.42498594 0.42494721 0.42702143 0.42836814  [0.42790025 0.42552534 0.42305778 0.42319051 0.42496695 0.42645839  [0.42697849 0.42434483 0.42154831 0.42180753 0.42318466 0.42435914  [0.42642263 0.42314516 0.42067778 0.42064949 0.42125724 0.42242715  [0.42547189 0.42260385 0.42037163 0.41977151 0.41930503 0.42059474  [0.42457665 0.42257894 0.42005962 0.41886955 0.41792893 0.41935383  [0.42420621 0.42249567 0.42008166 0.41833237 0.4175014  0.41835068  [0.42364156 0.42242975 0.42005474 0.41790344 0.41731561 0.41741042  [0.4231792  0.42199792 0.42006037 0.41769008 0.41685999 0.41679135  [0.42283564 0.42194327 0.42012014 0.41729961 0.416417   0.41633152  [0.4224698  0.42210881 0.42023181 0.41700237 0.41593933 0.41598625  [0.42229072 0.42237519 0.42034572 0.41676139 0.41565239 0.4156043   [0.42230572 0.42258009 0.42042929 0.41678605 0.41536841 0.41528044  [0.42248549 0.42281361 0.4205294  0.41686743 0.41504477 0.41491713  [0.42253307 0.42302623 0.42074372 0.41670835 0.41477784 0.41455741  [0.42261982 0.4230881  0.42087334 0.41657329 0.41458856 0.41434147  [0.42281602 0.42300183 0.42085261 0.41634232 0.41440032 0.41422947  [0.42298146 0.42294353 0.4206956  0.41596371 0.41431511 0.41411233  [0.42302416 0.42293242 0.42046758 0.4156373  0.4142353  0.41398826  [0.423093   0.42286562 0.42026291 0.41549609 0.41415739 0.41383509  [0.42317801 0.42276127 0.42010255 0.41532899 0.414142   0.41375422\n",
      " 0.65072469 0.65458818 0.65903442 0.66492026 0.66943805 0.67384161   0.4971327  0.49621616 0.49535817 0.49373015 0.49579659 0.49567163   0.46555397 0.46722181 0.4669676  0.46746847 0.46892685 0.46805763   0.45099156 0.45291656 0.45342005 0.45396755 0.45561687 0.4562242    0.44383126 0.4444379  0.44384559 0.44499631 0.44618847 0.45027189   0.43872103 0.4372908  0.43634506 0.43908254 0.44132086 0.44349419   0.43375856 0.43115783 0.43231151 0.43363402 0.43620263 0.43831355   0.43027757 0.42772179 0.42804487 0.42992523 0.43280321 0.43411004   0.42813473 0.4247723  0.42552332 0.42737861 0.42921754 0.43042985   0.42569183 0.42243624 0.42371836 0.42540585 0.42533868 0.42747319   0.42328811 0.42091736 0.42175158 0.42339906 0.42335434 0.42490209   0.42149773 0.41980355 0.42016958 0.42179941 0.42178315 0.42342834   0.42017208 0.41864885 0.41875728 0.42055263 0.42062734 0.42210878   0.41920474 0.41766324 0.41784486 0.41950443 0.41993111 0.42099889   0.41831254 0.4170439  0.41700838 0.41872157 0.41917801 0.42025362   0.41759695 0.41635668 0.41635923 0.41807185 0.41841062 0.4197378    0.41671424 0.41576961 0.41583999 0.41757299 0.41793439 0.41956592   0.41613624 0.41529916 0.41546771 0.41715788 0.41746598 0.4192651    0.41582502 0.41481998 0.41515007 0.41671119 0.41690723 0.4187064    0.41532862 0.41441715 0.41504271 0.41631497 0.41630714 0.41797829   0.41493254 0.41419557 0.4148276  0.41588991 0.41571865 0.41730182   0.41434887 0.41389025 0.41443065 0.41538122 0.41514544 0.41683098   0.41378605 0.41351067 0.41408599 0.41486361 0.41473792 0.41645254   0.41330253 0.41314955 0.41367856 0.41438467 0.41438977 0.41624971   0.41285951 0.41275581 0.41338128 0.41397138 0.41408526 0.41621591   0.41242838 0.41238215 0.41307383 0.41352003 0.41395286 0.41625211   0.41209632 0.41197002 0.41271337 0.41316655 0.41390909 0.41627638   0.41186842 0.41167782 0.4123917  0.41294572 0.41401844 0.41635826   0.41171724 0.41153306 0.41214268 0.41289439 0.41414444 0.41653007   0.41156386 0.41148471 0.41197283 0.41291398 0.41433395 0.41658927\n",
      " 0.67758977 0.68101045 0.68564237 0.68703677 0.68783138 0.69063302   0.4969183  0.49823628 0.50114352 0.50220807 0.50578506 0.5041192    0.46801357 0.46949675 0.47064086 0.47368332 0.47418511 0.47674964   0.45805648 0.45682425 0.45758371 0.45987302 0.46112421 0.4603754    0.45092677 0.44994445 0.45041024 0.4508801  0.4497382  0.4516       0.44452251 0.44420743 0.44349507 0.44375497 0.44339333 0.4454251    0.43883906 0.43869313 0.43822623 0.43804788 0.43887758 0.44028612   0.43435286 0.43462909 0.43455407 0.43376083 0.43513015 0.43733016   0.43098389 0.431676   0.43156186 0.43087468 0.43341809 0.43567169   0.42832701 0.42885625 0.42922836 0.42960232 0.43212816 0.4344668    0.42638224 0.42660802 0.42763536 0.42886884 0.43104604 0.43369413   0.42451929 0.42526097 0.42668353 0.42836034 0.43036447 0.43263604   0.42317677 0.42451584 0.4257863  0.42742631 0.42958993 0.43180682   0.42232742 0.4237592  0.42472058 0.42649902 0.42901696 0.43110477   0.4217102  0.42309043 0.42398949 0.42588825 0.4287587  0.43059737   0.42128103 0.42246551 0.42340011 0.42545935 0.42820808 0.43010851   0.42083421 0.42178776 0.4228277  0.42487579 0.42768229 0.42955999   0.42024105 0.42121219 0.42237851 0.42439015 0.42728768 0.4291985    0.41977476 0.42067717 0.42190164 0.4241719  0.42677839 0.42889417   0.41929458 0.42019499 0.42172068 0.42389262 0.42616541 0.42876946   0.41886462 0.41982996 0.42148621 0.42346973 0.42566614 0.42878023   0.41836492 0.41955045 0.42127063 0.42313875 0.42559837 0.42861924   0.41792607 0.41929959 0.4210422  0.42306359 0.42554231 0.4283729    0.41753101 0.41906872 0.42086315 0.42300662 0.42539093 0.4281696    0.41723165 0.41894254 0.42069715 0.42291512 0.42520223 0.42799155   0.41704614 0.41894669 0.42063803 0.42273088 0.42512348 0.4278298    0.41695342 0.41896266 0.42049247 0.42253391 0.42506696 0.42766468   0.41695023 0.4190548  0.42041771 0.42243521 0.42492294 0.42755187   0.41705089 0.41905318 0.42045293 0.42237257 0.42495142 0.42753283   0.41725758 0.41902681 0.42059529 0.42236919 0.42505878 0.42753902\n",
      " 0.69499221 0.70191086]                                              0.50688597 0.51044035]                                              0.47708244 0.47857258]                                              0.46098517 0.4632405 ]                                              0.45227211 0.45339443]                                              0.44730602 0.44743448]                                              0.44242964 0.44458472]                                              0.43986927 0.44268085]                                              0.4380008  0.44139593]                                              0.43672513 0.4406403 ]                                              0.43565896 0.43956922]                                              0.43489227 0.43880784]                                              0.43456398 0.43802763]                                              0.43445437 0.43736918]                                              0.4339246  0.43718579]                                              0.43344314 0.43709487]                                              0.43302349 0.43691169]                                              0.43265584 0.4367935 ]                                              0.43234929 0.43656282]                                              0.43206514 0.43620777]                                              0.43174408 0.43608598]                                              0.431419   0.4360077 ]                                              0.43113499 0.43593771]                                              0.43106711 0.43584682]                                              0.43109108 0.43582452]                                              0.43113551 0.43582829]                                              0.43115052 0.4358877 ]                                              0.43121618 0.43604517]                                              0.43131217 0.43628537]                                              0.43144961 0.43650241]\n",
      "[0.64231398 0.64040083 0.63837406 0.64036104 0.64207835 0.64653808  [0.48945547 0.48809463 0.48978861 0.49240359 0.49153379 0.49268692  [0.46385088 0.46512843 0.46716575 0.46720367 0.46581946 0.46512694  [0.45274236 0.45455786 0.45278722 0.452418   0.45091098 0.45047027  [0.44581793 0.44583625 0.44385941 0.44293939 0.44220436 0.44180929  [0.43896254 0.43957099 0.4378845  0.43764838 0.437425   0.438075    [0.43516115 0.43600208 0.4337281  0.43303968 0.4345947  0.43514756  [0.43292225 0.43310103 0.43015361 0.42960318 0.43164935 0.43228887  [0.43106036 0.42995617 0.42690011 0.42710688 0.42930245 0.43019992  [0.42918985 0.42742517 0.42500343 0.42494358 0.42696595 0.42834163  [0.42795548 0.42559361 0.42306973 0.42318665 0.42495562 0.42642552  [0.42705185 0.42440348 0.42158745 0.42181124 0.42316741 0.42431514  [0.4265126  0.42320173 0.42073511 0.42067405 0.42122772 0.42237295  [0.42552244 0.42270621 0.42042291 0.41979964 0.41926497 0.42058039  [0.42462315 0.42268004 0.42011448 0.4189026  0.41788029 0.41934641  [0.42428093 0.42260469 0.42016123 0.41840409 0.4174901  0.41834547  [0.42375018 0.422521   0.42010227 0.4179741  0.4173068  0.41743772  [0.42327647 0.42209436 0.42010279 0.41777301 0.41687586 0.41686041  [0.42293172 0.42205598 0.42024747 0.41737621 0.41645143 0.41635959  [0.4225715  0.42222181 0.42037882 0.4170882  0.41597673 0.41604183  [0.42239605 0.42246989 0.42051501 0.4168322  0.41570296 0.41569931  [0.42240014 0.42272277 0.42057396 0.41685735 0.41538645 0.41537361  [0.42260272 0.42297646 0.42068683 0.41694236 0.41508649 0.4150131   [0.4226862  0.42316239 0.42090708 0.41681883 0.41483963 0.41466572  [0.42276759 0.42326886 0.42103214 0.41666853 0.41464806 0.41447321  [0.42297851 0.42318861 0.42101227 0.41648661 0.41446202 0.41435422  [0.42317462 0.4231483  0.42089935 0.4161173  0.41436197 0.41423396  [0.42320252 0.42314394 0.42063575 0.41581385 0.41428968 0.41409517  [0.42329501 0.42309954 0.42043823 0.41566038 0.41421556 0.41398587  [0.42339623 0.4229745  0.42030188 0.41549036 0.414193   0.41389512\n",
      " 0.6506081  0.65448654 0.65890286 0.66485481 0.66936522 0.67369862   0.49709207 0.49613074 0.49529994 0.49366406 0.4957534  0.49558538   0.46551039 0.46716035 0.46688129 0.46739395 0.46889022 0.46798977   0.45091984 0.45286635 0.45336487 0.45390652 0.45555502 0.45618098   0.44377681 0.44438185 0.44379717 0.44495459 0.44617507 0.45021844   0.43864631 0.43722914 0.43630753 0.43903648 0.44126091 0.4434292    0.43369814 0.43111229 0.43225661 0.43357388 0.43613114 0.43828136   0.43024468 0.42770679 0.42798374 0.42988186 0.43276301 0.43407461   0.42808953 0.42471798 0.42549793 0.42736657 0.42914585 0.4304071    0.42567063 0.42243069 0.42368125 0.42535576 0.42530192 0.42744934   0.42327745 0.42092123 0.42172675 0.42336707 0.42330793 0.42488313   0.42150534 0.41980152 0.42015663 0.42179223 0.4217581  0.42343192   0.4201657  0.41865764 0.41875345 0.42051015 0.42061886 0.4221095    0.41921464 0.41767745 0.41785256 0.41946961 0.41992203 0.42098964   0.41831295 0.41706606 0.41701402 0.41868498 0.41915977 0.42023977   0.41759297 0.41638748 0.41635308 0.41808466 0.41841232 0.41975477   0.41677395 0.41581985 0.41583109 0.41762571 0.41794842 0.41954793   0.41620025 0.41531882 0.41545179 0.41715987 0.41745334 0.41926793   0.41586196 0.41484177 0.4151527  0.4167039  0.41693676 0.41868072   0.41540465 0.41444049 0.41505947 0.41632135 0.41631502 0.41795386   0.41500861 0.41421149 0.41483131 0.41590684 0.4157081  0.41727981   0.41446533 0.41392584 0.41448427 0.41537622 0.41511787 0.41681245   0.41389858 0.41354209 0.41407407 0.41488823 0.41472027 0.41640789   0.4133903  0.41318914 0.4137073  0.41438246 0.41435486 0.41621804   0.41293535 0.41283461 0.41337921 0.41397243 0.41406117 0.41620196   0.41253111 0.41244129 0.41305297 0.41352566 0.41393558 0.4162283    0.41219403 0.41199818 0.4126843  0.41314269 0.41390172 0.41623452   0.41194739 0.4116853  0.41236473 0.41290208 0.4139749  0.41630687   0.41180009 0.4115758  0.41214422 0.41284582 0.41410523 0.41647947   0.41167606 0.41153385 0.41197029 0.41288977 0.41430303 0.41652804\n",
      " 0.67753239 0.68084162 0.6855549  0.68688497 0.6876907  0.69046718   0.4968191  0.4981593  0.5010778  0.50212909 0.50568457 0.50406285   0.46795396 0.46943587 0.4706067  0.47363473 0.47411144 0.47669761   0.45800596 0.45675341 0.45756412 0.45982041 0.46107144 0.46028144   0.45086199 0.44988192 0.45036288 0.450817   0.4496643  0.45152414   0.44448209 0.44417011 0.44345726 0.44369402 0.44330511 0.4453767    0.43879173 0.43865115 0.4382118  0.43798381 0.43880535 0.4402432    0.43433169 0.43459267 0.43450245 0.43370618 0.43507209 0.43729912   0.43094289 0.43164374 0.43149887 0.43081601 0.43337235 0.43564105   0.42829387 0.42882249 0.42917056 0.42955899 0.43205936 0.43444715   0.42637796 0.42659972 0.42756324 0.42884127 0.43099458 0.43365894   0.42448443 0.42522784 0.42660852 0.42830951 0.4303147  0.43260317   0.42315911 0.42449706 0.42571609 0.42740721 0.42952229 0.43176228   0.42235173 0.42372654 0.42469919 0.42650019 0.42895639 0.43108784   0.42171002 0.42306634 0.42393484 0.42585521 0.42873224 0.43055755   0.42126692 0.42245207 0.42335697 0.42545987 0.42818923 0.43008285   0.42082633 0.42175976 0.42281343 0.42484524 0.42765865 0.42955196   0.42022589 0.42116714 0.42237165 0.42437477 0.42724642 0.42918669   0.41973743 0.42064629 0.42187915 0.42414216 0.42676434 0.4288556    0.4192928  0.42013464 0.4216994  0.42387028 0.426148   0.42875596   0.41887589 0.41980885 0.42147659 0.42345231 0.42564885 0.4287656    0.41833588 0.41952292 0.42125067 0.42314643 0.42558492 0.42859208   0.41790176 0.4192252  0.42102349 0.42307046 0.42552911 0.42835795   0.41752781 0.41901095 0.42081157 0.42297512 0.42535912 0.42811703   0.41719783 0.4189534  0.4206657  0.42290132 0.42518774 0.42794583   0.41700152 0.41890887 0.42059966 0.42270511 0.42510524 0.42775284   0.41695247 0.41895617 0.42044875 0.4224986  0.425034   0.42763398   0.41694919 0.41903265 0.42037552 0.42238892 0.42487921 0.42751178   0.41704091 0.41903101 0.42039187 0.4223582  0.42490637 0.42748381   0.41724483 0.419022   0.42053751 0.42233881 0.42501971 0.42746962\n",
      " 0.69489099 0.70176716]                                              0.50682978 0.51035655]                                              0.47700328 0.47851767]                                              0.46093649 0.46318331]                                              0.45223563 0.45333112]                                              0.44724478 0.44735596]                                              0.44237007 0.44453909]                                              0.43983197 0.44262701]                                              0.43798816 0.44136688]                                              0.43669431 0.44059156]                                              0.4356216  0.43953336]                                              0.43485025 0.43878852]                                              0.43455046 0.43801726]                                              0.4344235  0.43735576]                                              0.4338811  0.43715389]                                              0.43342528 0.43707987]                                              0.43302517 0.43687983]                                              0.43264256 0.43672745]                                              0.43235273 0.43650954]                                              0.43206659 0.43616118]                                              0.43173408 0.43599803]                                              0.43138556 0.43594061]                                              0.43111156 0.43586163]                                              0.43103038 0.43577155]                                              0.43105468 0.43573832]                                              0.43108187 0.43577052]                                              0.43110493 0.43582037]                                              0.43114747 0.4359464 ]                                              0.4312434  0.43618425]                                              0.43136899 0.43638949]\n",
      "[0.6412286  0.63908841 0.63710367 0.63869421 0.63999371 0.64479237  [0.4885358  0.48747391 0.48906587 0.4918806  0.49081482 0.49212373  [0.46334345 0.46486699 0.46707521 0.46675059 0.46541864 0.46433501  [0.45251781 0.45452674 0.45262649 0.45204242 0.45053798 0.45043853  [0.44596611 0.4458576  0.44397509 0.4429033  0.44219495 0.44193221  [0.4389654  0.43986939 0.43803556 0.43770359 0.43778822 0.43854487  [0.43578985 0.43631982 0.43410491 0.43338433 0.43492145 0.43554075  [0.43381079 0.43311448 0.4308728  0.4300249  0.43211008 0.43286801  [0.43167806 0.43046326 0.4276664  0.42739629 0.42990854 0.43078318  [0.42980844 0.42843557 0.42592153 0.42566383 0.42759259 0.42908445  [0.42875126 0.42693512 0.42414972 0.42396475 0.42586385 0.42721747  [0.42806672 0.42538059 0.4229519  0.42271519 0.42423531 0.4251391   [0.42733927 0.42409439 0.42220289 0.42180949 0.4224694  0.42336523  [0.42627649 0.42396193 0.42186422 0.42092912 0.42067932 0.42168086  [0.42544123 0.42390977 0.42148689 0.42013192 0.41948516 0.42042803  [0.42516562 0.42385403 0.42145087 0.41961221 0.41913492 0.41946827  [0.42477158 0.42377683 0.42151809 0.4192686  0.41910753 0.41866702  [0.42445633 0.42326977 0.42171037 0.41916515 0.41877918 0.41805459  [0.42421716 0.42324448 0.42196908 0.41904783 0.41828486 0.41771658  [0.42401981 0.42347348 0.42221055 0.41882541 0.41798894 0.41754017  [0.42393049 0.42380359 0.42251188 0.41898411 0.41769576 0.41720404  [0.42402406 0.42424122 0.42268827 0.41923727 0.41743877 0.41694675  [0.42406537 0.42452041 0.42285601 0.41929544 0.41705479 0.41650813  [0.42416844 0.42480905 0.42303488 0.41917449 0.41664471 0.41619945  [0.42427474 0.42497117 0.42303178 0.41896993 0.41633907 0.41595575  [0.42438958 0.42501296 0.42306605 0.41879521 0.41606923 0.41582087  [0.42463124 0.42494769 0.42313076 0.41859113 0.41599897 0.41570415  [0.42482854 0.42493981 0.42309619 0.41831583 0.41604149 0.41566061  [0.42494274 0.42493216 0.42300536 0.41820249 0.41616731 0.4157348   [0.42504517 0.42491565 0.42291909 0.4180211  0.4162941  0.41567981\n",
      " 0.64885456 0.65312583 0.65744823 0.66323275 0.6679329  0.67234811   0.49651788 0.49525217 0.4945394  0.49317491 0.49515032 0.4950008    0.46513663 0.46686415 0.46666175 0.46690617 0.46835399 0.46766499   0.45047017 0.4525664  0.45305853 0.45372292 0.45551351 0.4561306    0.44391881 0.44422462 0.44352796 0.44496895 0.44621049 0.45027009   0.43874204 0.43710657 0.43635152 0.43878315 0.44125469 0.44322629   0.4339013  0.43137285 0.43226854 0.43368804 0.4361196  0.43834931   0.43074126 0.42811837 0.42808544 0.430106   0.43286491 0.43406849   0.42875443 0.42540381 0.42600872 0.42767149 0.42921185 0.43044302   0.42633984 0.42294349 0.42437086 0.42583588 0.42572557 0.42749465   0.4241694  0.4214236  0.4224773  0.42393436 0.42390643 0.42516168   0.42242194 0.42065599 0.42072181 0.42247261 0.42253824 0.42391031   0.42121738 0.41981654 0.41940777 0.42142848 0.42164656 0.42288903   0.42034262 0.41887598 0.41843759 0.42064297 0.42127002 0.42188574   0.41958499 0.41820588 0.41785232 0.42005418 0.42070893 0.42117603   0.41858289 0.41752878 0.41741738 0.41968528 0.42000986 0.42075944   0.41774983 0.41692073 0.41710004 0.41940471 0.41948586 0.42062044   0.41721894 0.41639692 0.41692572 0.41919276 0.41908513 0.42036357   0.41696758 0.41587008 0.41674501 0.41882855 0.41858634 0.41983322   0.41664574 0.41549491 0.41673947 0.41857054 0.41799418 0.41924327   0.41632561 0.41524583 0.41665612 0.41823082 0.41750207 0.41864058   0.41594473 0.41490417 0.41646436 0.41782828 0.41688667 0.41811069   0.41545314 0.41458054 0.41617896 0.41733054 0.41654701 0.41774372   0.41501518 0.41424653 0.41588349 0.41681265 0.41622633 0.41738783   0.41463092 0.4139948  0.41556236 0.4162342  0.41592311 0.4172196    0.41426486 0.4138172  0.41515046 0.41567815 0.41570207 0.41720265   0.41392122 0.41363785 0.41479727 0.415303   0.415561   0.41735107   0.413673   0.41346749 0.41449733 0.41495556 0.41552834 0.4174671    0.41348597 0.41337576 0.41427704 0.41487796 0.4155613  0.4176123    0.41333948 0.41331214 0.41417191 0.41486218 0.41570155 0.41783105\n",
      " 0.67618666 0.67950884 0.6839053  0.6850165  0.68591184 0.68814264   0.49614677 0.49717096 0.50008578 0.50140908 0.50491474 0.50329232   0.46751493 0.46896612 0.46996424 0.47285944 0.47331681 0.47609486   0.45792083 0.45637073 0.45711862 0.45928295 0.46055895 0.45977826   0.45058898 0.44965986 0.45010221 0.45048739 0.44940655 0.45110613   0.44430167 0.44401293 0.44327954 0.44369247 0.44290025 0.44505049   0.43872651 0.43868051 0.43842025 0.437959   0.43847027 0.44001143   0.43448339 0.43473784 0.43463274 0.43374931 0.43487869 0.43741925   0.43122116 0.43195182 0.4316881  0.43099139 0.43338074 0.43597289   0.4286986  0.42932612 0.4296457  0.42978294 0.43236353 0.43484678   0.42683872 0.42720333 0.42803402 0.42914893 0.43138579 0.43381517   0.42505102 0.42578936 0.42702344 0.42846563 0.43062129 0.43269922   0.42384425 0.42500907 0.42623076 0.4273787  0.42978085 0.43195564   0.42310341 0.42424955 0.42520776 0.42660366 0.42926964 0.43135089   0.42255942 0.42375435 0.42447258 0.42598221 0.42911631 0.43094184   0.42211455 0.42307838 0.42377631 0.42558471 0.42873446 0.43051349   0.42175709 0.42253558 0.42321712 0.42494227 0.42826995 0.43000753   0.42111847 0.4219354  0.42275634 0.42458775 0.42788817 0.42962179   0.42053083 0.4214011  0.42245063 0.42426787 0.42736945 0.42940834   0.4200828  0.42106555 0.42231988 0.42401148 0.4266236  0.42938215   0.41974033 0.42075949 0.42214555 0.42356444 0.42620568 0.42942796   0.4193578  0.42058638 0.4220792  0.42337813 0.42614438 0.42937316   0.41904394 0.42037034 0.42178023 0.42339774 0.42607944 0.42919388   0.41879694 0.42023261 0.4216253  0.42344209 0.42599517 0.4289521    0.41852022 0.42020732 0.42139049 0.42345212 0.42583592 0.42880028   0.41840999 0.42005255 0.42129966 0.42334358 0.42572206 0.42871634   0.41830516 0.4199606  0.42123871 0.42331114 0.42568208 0.42856318   0.41835479 0.42006315 0.42123061 0.42331371 0.42561612 0.42846392   0.41842492 0.42015464 0.42129104 0.42329507 0.42553059 0.42843414   0.41853937 0.42023747 0.42140851 0.42326626 0.42557189 0.42845026\n",
      " 0.69321187 0.69999827]                                              0.50599769 0.50933049]                                              0.47623348 0.47806147]                                              0.46068943 0.46280311]                                              0.45196135 0.45318635]                                              0.44688618 0.44739561]                                              0.44229652 0.44454454]                                              0.4396625  0.44265867]                                              0.43788437 0.44140037]                                              0.43646931 0.44062136]                                              0.43549022 0.43956266]                                              0.43467689 0.43875731]                                              0.43447159 0.43811998]                                              0.4345551  0.43766446]                                              0.43434849 0.43760917]                                              0.43394333 0.43760714]                                              0.43360627 0.43744498]                                              0.43336101 0.43723233]                                              0.43313047 0.43702026]                                              0.43299816 0.4368025 ]                                              0.43266924 0.4366219 ]                                              0.43234257 0.43654623]                                              0.43207053 0.43643277]                                              0.43193388 0.43633573]                                              0.43179379 0.43627881]                                              0.43176993 0.43626107]                                              0.4318187  0.43627241]                                              0.43183403 0.43640482]                                              0.43191851 0.43662943]                                              0.43205327 0.43684826]\n",
      "[0.63626189 0.63492981 0.63284594 0.63364049 0.63319751 0.64021553  [0.48655953 0.48582765 0.48787024 0.49005023 0.48971636 0.49097865  [0.46271503 0.46417563 0.46636694 0.46604756 0.46350167 0.46383358  [0.4527499  0.45398964 0.45285307 0.45168381 0.44950256 0.45035973  [0.44607882 0.44544735 0.44492992 0.44357211 0.44172025 0.44289624  [0.4400216  0.43961646 0.43884719 0.43850301 0.43767634 0.43827778  [0.43725374 0.43625951 0.43533658 0.43444276 0.43517062 0.43492792  [0.4349333  0.43393076 0.43237942 0.43062429 0.43206908 0.43238313  [0.43284573 0.43141792 0.42917791 0.42800751 0.42974879 0.43081658  [0.4317061  0.42999427 0.42732907 0.42648989 0.42763116 0.42884982  [0.43037073 0.42782539 0.42547765 0.42509634 0.42607566 0.42670068  [0.42920126 0.42696099 0.42444967 0.42384292 0.42464618 0.42452299  [0.42840702 0.42621969 0.4236837  0.42277444 0.42350117 0.42252275  [0.42759658 0.4255666  0.42317886 0.42181104 0.42228756 0.4207157   [0.42710415 0.42516045 0.42304263 0.42096395 0.42139643 0.41971759  [0.42647885 0.42502311 0.42310258 0.42047713 0.4209205  0.419376    [0.42567899 0.42466019 0.42317328 0.42018157 0.42058402 0.41908661  [0.42517448 0.42443922 0.42323912 0.42008852 0.42029051 0.41868925  [0.42509431 0.42429285 0.42331183 0.42007886 0.42000152 0.41854456  [0.42515167 0.42414098 0.4233539  0.42024616 0.41987529 0.41852793  [0.42519492 0.42434834 0.42351704 0.42044578 0.42010832 0.41858633  [0.42531525 0.42464983 0.42361116 0.42059875 0.42010295 0.41859814  [0.42543525 0.42499567 0.42379408 0.42066665 0.41998006 0.41853199  [0.42556746 0.42539673 0.42398289 0.42088983 0.41982889 0.41833728  [0.42582499 0.42569587 0.42394651 0.42105979 0.41967707 0.41822443  [0.42593564 0.42587612 0.42386002 0.42112454 0.41951797 0.41808628  [0.42592261 0.4259698  0.42384195 0.4210829  0.4193975  0.41785978  [0.42590882 0.42599228 0.42388794 0.42109835 0.4192487  0.4178323   [0.42593087 0.42593343 0.42396566 0.42125937 0.4192222  0.41786723  [0.42594929 0.42585258 0.42405856 0.42131613 0.41917289 0.41789532\n",
      " 0.64383754 0.64843606 0.65242126 0.65665711 0.66194455 0.66641186   0.49346089 0.49292367 0.49368133 0.49161006 0.49330614 0.49338638   0.46453109 0.46594097 0.46501123 0.46560389 0.46733131 0.46676684   0.44989994 0.45159233 0.45135293 0.45201462 0.45453743 0.45546544   0.44280305 0.44346481 0.44211569 0.44376535 0.44608241 0.44989029   0.4380212  0.43592764 0.4351767  0.43788579 0.44199239 0.44333532   0.43355832 0.43144823 0.43111058 0.43307729 0.43665638 0.43847226   0.42981647 0.42794819 0.42766457 0.43023986 0.43363871 0.43376911   0.42787069 0.42570312 0.42562399 0.42834061 0.42988291 0.43026182   0.42606045 0.42373122 0.42446946 0.42606031 0.42669074 0.42723216   0.42426304 0.42265077 0.42315776 0.42441934 0.42440115 0.42520708   0.42243784 0.42189289 0.42191855 0.42302319 0.42317727 0.42414405   0.42097502 0.42112954 0.42105833 0.42208783 0.42234313 0.42370494   0.42025822 0.42045954 0.42020372 0.42145871 0.42204999 0.42318785   0.41978924 0.419825   0.4195891  0.42110214 0.42175601 0.42288958   0.41922172 0.41912265 0.41896206 0.42080678 0.42147139 0.42256121   0.41866252 0.41875555 0.4185013  0.4208772  0.42136619 0.42233995   0.41835719 0.41829244 0.41822904 0.42075146 0.4210102  0.4219608    0.41813844 0.41794015 0.41786251 0.42053664 0.42060168 0.4215266    0.41813378 0.41770379 0.41763874 0.4203537  0.42028718 0.42111662   0.41795221 0.4173718  0.41750784 0.42025476 0.42012772 0.42070232   0.41770453 0.41715273 0.41743841 0.41994957 0.41999189 0.42022689   0.41746978 0.41693681 0.41747027 0.41968922 0.41956119 0.41976875   0.4171988  0.41681618 0.41741788 0.41942676 0.41909012 0.41955059   0.41693305 0.41668966 0.41739018 0.41913197 0.41871978 0.41953278   0.41669855 0.41664043 0.41734437 0.41888065 0.4183019  0.41959926   0.41658944 0.41664811 0.41745919 0.41859602 0.41792619 0.41978277   0.41659766 0.41657003 0.41768964 0.41828834 0.41770502 0.41999288   0.41671315 0.41657718 0.41788087 0.41810345 0.417735   0.42024209   0.41687622 0.41660561 0.41803292 0.41798159 0.41785777 0.42046051\n",
      " 0.67014232 0.6725752  0.67757316 0.6790122  0.67960528 0.6805538    0.4940466  0.49486939 0.49781583 0.49993659 0.50237573 0.50080445   0.46661188 0.46803391 0.46930987 0.47117008 0.47275581 0.47437414   0.45759117 0.45560751 0.45665719 0.45832469 0.45920712 0.45962302   0.44982498 0.44918074 0.44954787 0.44993672 0.4489148  0.45045054   0.443466   0.44204962 0.44285425 0.44298036 0.44235007 0.44483297   0.43740315 0.43729776 0.43797818 0.43832327 0.43842899 0.43995745   0.43292039 0.43415577 0.43469638 0.43426481 0.43442228 0.4373555    0.42979269 0.43172551 0.43195036 0.43169129 0.43313367 0.43639406   0.42766951 0.42942626 0.43012505 0.4303348  0.43238379 0.43584135   0.42626684 0.42727827 0.42874259 0.42945536 0.43164345 0.43454945   0.42518435 0.42586648 0.42773438 0.42858328 0.43095395 0.43353809   0.424316   0.42487861 0.42699034 0.42774388 0.43011591 0.43280877   0.42401025 0.42462999 0.42652446 0.42723472 0.42964152 0.43246954   0.42384568 0.42425097 0.42586861 0.42665011 0.42925677 0.43219747   0.42361551 0.42401667 0.42533596 0.4263354  0.42888203 0.43210687   0.42328575 0.4236023  0.42472858 0.42593325 0.42855527 0.43166871   0.42283366 0.42310806 0.42422183 0.42561865 0.42824509 0.43125285   0.42240631 0.42268464 0.42377614 0.42528317 0.42794673 0.43106682   0.42208441 0.42243499 0.42344028 0.42502839 0.42775364 0.43108778   0.421775   0.422177   0.42327144 0.42474597 0.42769847 0.43118047   0.42151391 0.42191988 0.42325583 0.42465333 0.42772167 0.43099373   0.42131677 0.42176375 0.42326236 0.42469175 0.42785911 0.43080532   0.42127498 0.42175169 0.4232471  0.42473273 0.42768796 0.43075339   0.42128702 0.42167615 0.42330561 0.42477484 0.42753994 0.43083792   0.42144126 0.42170123 0.42336558 0.42488249 0.42735212 0.43094565   0.42162541 0.42184351 0.42340903 0.42504003 0.42729419 0.43100717   0.42187066 0.42189054 0.42345553 0.42502474 0.42724795 0.43111378   0.42210985 0.42213331 0.42356402 0.42492883 0.42717281 0.43123522   0.42232447 0.42229748 0.42368114 0.4249566  0.42712049 0.43131359\n",
      " 0.68671204 0.69564325]                                              0.50389735 0.50699521]                                              0.47493679 0.47679078]                                              0.45969026 0.4616907 ]                                              0.45178643 0.45255363]                                              0.44691947 0.44785178]                                              0.44268623 0.44487384]                                              0.44019537 0.44297058]                                              0.43874635 0.44205878]                                              0.43723192 0.44136983]                                              0.43640618 0.44077084]                                              0.43623357 0.44043511]                                              0.43614964 0.44019378]                                              0.4362447  0.44020555]                                              0.43641809 0.43988777]                                              0.43617254 0.43951846]                                              0.4359308  0.43920456]                                              0.43574209 0.43887746]                                              0.43549855 0.43865949]                                              0.43518481 0.43839413]                                              0.43495945 0.43791747]                                              0.43475852 0.43775143]                                              0.43454689 0.43760529]                                              0.43454326 0.43756892]                                              0.43454203 0.43754923]                                              0.43452197 0.43756415]                                              0.43452291 0.43767267]                                              0.43445959 0.43791209]                                              0.4344654  0.43806968]                                              0.4344933  0.43834388]\n",
      "[0.5999663  0.60213009 0.60223017 0.60015199 0.60243284 0.60429967  [0.49888274 0.49981345 0.50080669 0.50035098 0.50081923 0.50318574  [0.48377985 0.48305605 0.48447383 0.48525049 0.48615509 0.48841073  [0.47656977 0.47791541 0.47742733 0.47682626 0.47648386 0.47800032  [0.47385974 0.47383413 0.47446177 0.47266378 0.4727213  0.47421506  [0.4732665  0.47249647 0.47285621 0.4684939  0.47068057 0.4722186   [0.47206119 0.47071458 0.4702988  0.4693573  0.46948405 0.46992995  [0.47018844 0.47035609 0.46879198 0.46871685 0.46873782 0.46962963  [0.47041882 0.4698768  0.4678982  0.46805355 0.46798845 0.46818621  [0.47105998 0.46989351 0.46780041 0.46782952 0.46772778 0.46743639  [0.47031894 0.46955573 0.46811392 0.46721192 0.46697043 0.46649981  [0.46844928 0.4685293  0.46725335 0.46610251 0.46606963 0.46582019  [0.46722136 0.46779849 0.46677264 0.46580042 0.46480972 0.46484679  [0.4668414  0.46747698 0.46680518 0.46548783 0.46457496 0.46455068  [0.46658112 0.46716144 0.46672041 0.46512581 0.46446295 0.46451418  [0.46612433 0.46704607 0.46675728 0.46502932 0.4637367  0.46465343  [0.46591054 0.46726128 0.46712596 0.46465458 0.46317181 0.46500322  [0.46612731 0.46742241 0.46685519 0.4643721  0.46341556 0.46544407  [0.46635721 0.46741305 0.46658309 0.46431735 0.46408577 0.46578492  [0.4666697  0.46729401 0.46637421 0.46465997 0.46496678 0.46616943  [0.46689924 0.46729536 0.46644275 0.46528587 0.46569542 0.46654163  [0.46695558 0.46766185 0.466697   0.46604667 0.46625611 0.4673375   [0.46708378 0.46799603 0.46688885 0.46668514 0.46700256 0.46824372  [0.467244   0.46826738 0.46705938 0.46672072 0.46741717 0.46880571  [0.4671931  0.46844649 0.46737687 0.46692644 0.46789257 0.46908635  [0.46730998 0.46873311 0.46773302 0.46709036 0.46810621 0.46922816  [0.46757532 0.46916784 0.46778691 0.4670335  0.46835016 0.46947753  [0.46770095 0.46938505 0.46751924 0.46682862 0.46863665 0.46940092  [0.46773969 0.46961899 0.46752017 0.46665976 0.46879556 0.46954213  [0.46786187 0.46988012 0.46769345 0.46663077 0.46898229 0.46969034\n",
      " 0.60405571 0.60826938 0.61200003 0.61435355 0.61664529 0.6180589    0.50249303 0.50227873 0.5020819  0.50099096 0.5009166  0.50057236   0.48691771 0.48659779 0.48457234 0.4834297  0.48350803 0.483608     0.47964792 0.47844764 0.47675317 0.47628953 0.47632947 0.47687812   0.47426363 0.4735114  0.47285314 0.47188209 0.47231715 0.47208566   0.47188937 0.47096064 0.4708398  0.47038711 0.46968133 0.46921523   0.47007382 0.47007374 0.46944978 0.46951857 0.4680027  0.46659704   0.46909067 0.46819762 0.46889457 0.46743948 0.46656215 0.4637632    0.46825142 0.46659184 0.46662237 0.46520954 0.46480852 0.46093836   0.46727481 0.46488413 0.46595875 0.46475091 0.46425896 0.46054808   0.46618721 0.46399135 0.46560444 0.46533657 0.46410255 0.46121506   0.46621816 0.46387921 0.46496673 0.46514416 0.46436984 0.46185206   0.46663171 0.46412157 0.46448356 0.46512842 0.46385484 0.46203639   0.46696993 0.46451831 0.46431861 0.46494543 0.46378083 0.4617266    0.46673106 0.46476589 0.46381529 0.4647036  0.46367832 0.46166731   0.46672503 0.46502373 0.46334277 0.46418753 0.46363889 0.46132178   0.46693462 0.46466471 0.46317845 0.4639972  0.46319999 0.46117544   0.46686698 0.46459896 0.46307834 0.46371731 0.46349573 0.46106815   0.46690191 0.46443344 0.4631031  0.46344925 0.46409809 0.46107498   0.46716172 0.46480733 0.46320136 0.46330655 0.46423838 0.46146676   0.46750303 0.46525116 0.46335478 0.46356472 0.46435347 0.46178357   0.46796316 0.46594781 0.463201   0.46398462 0.46445671 0.46190391   0.46863626 0.46630317 0.46347987 0.46439022 0.4644081  0.46198501   0.46898354 0.46676462 0.4636899  0.4647152  0.46439746 0.46227212   0.46931924 0.4672331  0.46389181 0.46516575 0.46459801 0.46263916   0.46960792 0.46747386 0.46409701 0.46555237 0.46481102 0.46300698   0.46978457 0.46757187 0.46445187 0.46579057 0.46510863 0.46342208   0.46982247 0.46766094 0.46484936 0.46617086 0.4654955  0.46388625   0.46971198 0.46776184 0.46529449 0.46663229 0.46608449 0.46423486   0.46947395 0.46790205 0.46571357 0.46710706 0.46642394 0.46459357\n",
      " 0.61985524 0.62079796 0.62376919 0.627514   0.63125763 0.6365297    0.50011346 0.50121205 0.50138381 0.50208383 0.50279078 0.50330303   0.4837046  0.48544089 0.48691888 0.48497878 0.48648782 0.48775266   0.47563208 0.47749613 0.47669001 0.47679613 0.47884665 0.4812244    0.47239985 0.47257946 0.47143057 0.47363538 0.47344218 0.47707917   0.46933007 0.46883688 0.46986535 0.47225248 0.47185976 0.47439436   0.46788244 0.46647798 0.46736626 0.46920587 0.46988715 0.47268962   0.46517088 0.46477109 0.46618239 0.46731636 0.46900982 0.47153523   0.46288471 0.46385061 0.46582574 0.46692101 0.46904779 0.47167566   0.46194591 0.46285687 0.46538722 0.46679503 0.46925037 0.47161078   0.46186541 0.46244563 0.46482527 0.46676942 0.46984094 0.47116284   0.46183395 0.46258687 0.46425789 0.46643911 0.46991266 0.47109209   0.46149954 0.46281846 0.46427868 0.46624245 0.47003263 0.47155856   0.46133413 0.46262779 0.46426114 0.46636473 0.4701396  0.47181366   0.46092444 0.46287198 0.46425213 0.46675083 0.47034007 0.47149183   0.46074555 0.46303944 0.46438477 0.46683736 0.46998333 0.47114366   0.46075483 0.46306248 0.46437784 0.46686512 0.46952079 0.47053574   0.46057438 0.46306066 0.4641424  0.4666524  0.46914416 0.47011712   0.46067098 0.46290724 0.4638849  0.46663794 0.46861982 0.46983972   0.4608865  0.46299712 0.46409546 0.46659871 0.46790239 0.46949394   0.46120876 0.46320774 0.46423605 0.46664487 0.4672202  0.46914681   0.46170569 0.46370151 0.46424746 0.46653769 0.46669009 0.46882788   0.46216293 0.46412815 0.46429542 0.46620942 0.46644307 0.46856611   0.46277415 0.46465885 0.46440922 0.46590834 0.4662656  0.46831435   0.46316962 0.46518406 0.46439799 0.46561307 0.46613479 0.46815209   0.46343957 0.4657824  0.46441023 0.46532731 0.46601203 0.46805226   0.46382216 0.46624025 0.4643598  0.46510962 0.46593794 0.46799832   0.46414103 0.46649562 0.46445584 0.46497398 0.46590252 0.46799231   0.46450881 0.4666994  0.46468406 0.46500117 0.46585078 0.46789227   0.46473268 0.46672491 0.4650308  0.46501313 0.46591299 0.46781495\n",
      " 0.64470741 0.65067099]                                              0.50513863 0.50735268]                                              0.4888189  0.49132963]                                              0.48329472 0.48446001]                                              0.47911249 0.4808492 ]                                              0.4767269  0.47776242]                                              0.4754685  0.47677731]                                              0.4751542  0.47629665]                                              0.47503263 0.47505214]                                              0.47483447 0.47533102]                                              0.47459538 0.47532814]                                              0.47436911 0.47517047]                                              0.47440031 0.47526061]                                              0.47446341 0.47539277]                                              0.47429295 0.47539779]                                              0.47375782 0.4752334 ]                                              0.47332536 0.47483676]                                              0.47281866 0.47428278]                                              0.47221761 0.47378596]                                              0.47182806 0.47342576]                                              0.47144896 0.47321383]                                              0.47109212 0.47313369]                                              0.47073625 0.47287744]                                              0.47038052 0.47274728]                                              0.47009658 0.4727101 ]                                              0.46972457 0.47265416]                                              0.46954177 0.47247269]                                              0.4693987  0.47231317]                                              0.46930126 0.47225297]                                              0.46882676 0.47214917]\n",
      "[0.58707113 0.59139627 0.59474459 0.59490637 0.5945401  0.59545331  [0.49495459 0.49328175 0.49265757 0.49447797 0.49370657 0.49414679  [0.47367262 0.47699684 0.47892545 0.48165909 0.48132608 0.47909196  [0.47039426 0.47258069 0.47384204 0.47471822 0.47456156 0.47309528  [0.46621508 0.46791216 0.46800023 0.46993002 0.46978875 0.47129713  [0.46187723 0.46328506 0.465572   0.46637963 0.4666203  0.46896795  [0.46092188 0.46312779 0.46409989 0.46468237 0.46663518 0.46827582  [0.46005334 0.46238237 0.46339086 0.46553183 0.46682871 0.46817707  [0.46044639 0.46214461 0.46343137 0.46651788 0.46687134 0.46743803  [0.46171617 0.46345099 0.46433088 0.46638064 0.46651037 0.46546808  [0.4626858  0.46295347 0.46429785 0.46611686 0.46431713 0.46402418  [0.46281087 0.46296052 0.46424536 0.4655187  0.46392131 0.46291095  [0.46258867 0.46293018 0.46386225 0.46474731 0.46309733 0.46237364  [0.46180886 0.46295197 0.46293522 0.46448943 0.46258179 0.46160669  [0.46167341 0.46272895 0.46234586 0.46354177 0.4621545  0.46137336  [0.46260166 0.46244768 0.46151165 0.46251176 0.46208158 0.46161265  [0.463615   0.46256072 0.46171041 0.46253543 0.46219175 0.46157595  [0.4644001  0.46270247 0.4617797  0.46262873 0.46236488 0.46175669  [0.4647135  0.46270592 0.46175052 0.46297641 0.46298695 0.46222728  [0.46494396 0.46260196 0.46217405 0.46319922 0.46347818 0.46226147  [0.46517925 0.4625856  0.46221533 0.46334826 0.46402193 0.46243996  [0.46519401 0.46270061 0.46227186 0.46345568 0.46453728 0.46284564  [0.46539919 0.46284948 0.46248884 0.46353643 0.4653235  0.46352769  [0.4655511  0.46304048 0.46268756 0.46336668 0.46565115 0.46435334  [0.46568829 0.46311517 0.4627554  0.46347297 0.46605476 0.46494747  [0.46570456 0.46313527 0.46282314 0.46371128 0.4665426  0.4654355   [0.46573635 0.46308235 0.46308139 0.46412915 0.46688054 0.46597622  [0.46569746 0.46285378 0.46338027 0.46437314 0.46731165 0.46653751  [0.46560005 0.46284686 0.46334626 0.4645523  0.46767917 0.46706399  [0.46523879 0.46305817 0.46336223 0.46464366 0.46803117 0.46754203\n",
      " 0.59569515 0.59548878 0.59761878 0.60339425 0.60809484 0.60749787   0.49346214 0.49542219 0.49616325 0.4986783  0.4988348  0.49800023   0.47824436 0.477328   0.47580044 0.47552179 0.4772901  0.47801714   0.47237929 0.47157853 0.4699788  0.46910882 0.47015226 0.47053054   0.46889928 0.46855429 0.46695771 0.46731011 0.46735864 0.46836165   0.46866431 0.46777706 0.46581827 0.46594965 0.46635912 0.46660286   0.46795304 0.46697969 0.46529641 0.46642728 0.46540968 0.46436959   0.46613706 0.46589758 0.46347611 0.4653719  0.46384739 0.46269676   0.46448369 0.46408026 0.46267998 0.46429644 0.46355209 0.46217638   0.46247536 0.46228384 0.4627433  0.46335631 0.46405949 0.46271692   0.46163713 0.46129074 0.46248952 0.46304576 0.4637881  0.46230774   0.46130143 0.46000426 0.46201783 0.4635822  0.4629438  0.46272937   0.46112847 0.45949408 0.46113926 0.46353263 0.46274661 0.46254069   0.46108486 0.45948239 0.46062778 0.46362432 0.46293944 0.46282256   0.4607524  0.45937518 0.46057977 0.46297422 0.46355052 0.46315383   0.46066489 0.45934842 0.46040141 0.4626868  0.46415567 0.46362139   0.46080783 0.45951953 0.46053406 0.46283337 0.46448642 0.46361879   0.46082429 0.45981807 0.46095932 0.46313133 0.46492542 0.46418657   0.4610488  0.46040606 0.4613131  0.463487   0.46513062 0.46467267   0.46152746 0.46116061 0.46166216 0.46332666 0.4651772  0.46463258   0.46219909 0.46188249 0.46216531 0.46315    0.46481775 0.46456798   0.46304714 0.46224672 0.46268322 0.46329004 0.46443464 0.46463463   0.46396993 0.46286185 0.463254   0.46316547 0.46351495 0.46472802   0.46497509 0.46368399 0.46357279 0.46298849 0.46297089 0.46478813   0.46560701 0.46385847 0.46375224 0.46306588 0.46299278 0.46486968   0.46605649 0.46396688 0.46376886 0.46308187 0.46301098 0.46461448   0.46643042 0.4640979  0.46391961 0.46317752 0.46340479 0.46477083   0.46671201 0.46411703 0.4641273  0.46327663 0.46366335 0.46485581   0.46699426 0.46433126 0.46451782 0.46340966 0.46379182 0.46488615   0.46724122 0.46450281 0.46496195 0.46362503 0.46387384 0.46488807\n",
      " 0.61022292 0.60836352 0.60983047 0.61321549 0.6168518  0.62268997   0.49706393 0.4946305  0.49391694 0.49570016 0.49656269 0.49594027   0.47713179 0.48051538 0.48240844 0.48078741 0.48260628 0.48168599   0.47158339 0.47263129 0.47499692 0.47498867 0.47561886 0.47761113   0.46877121 0.46989809 0.47129775 0.47214476 0.47387027 0.47285327   0.466954   0.46755094 0.46776876 0.47028387 0.47179416 0.47038497   0.46458522 0.46555157 0.4645226  0.46788449 0.46929962 0.46899044   0.4637809  0.46351668 0.4632986  0.46748226 0.46900958 0.46797444   0.46332066 0.46362127 0.46295822 0.46709781 0.46860931 0.46663746   0.46339959 0.4632688  0.46264289 0.46660493 0.46837351 0.46652447   0.46378858 0.46328947 0.46249783 0.46628826 0.46813135 0.46639867   0.46368615 0.46342753 0.46230179 0.4658019  0.46802663 0.46680481   0.46425734 0.46427523 0.46309266 0.46562628 0.46806761 0.46729395   0.46471215 0.46470151 0.46400319 0.46617674 0.46819838 0.46726099   0.46540319 0.46517096 0.46447024 0.46696144 0.46861625 0.4670059    0.46556397 0.46558031 0.46463969 0.46743604 0.46847866 0.46690766   0.46576739 0.46578245 0.46491279 0.4677341  0.46875688 0.46705845   0.4662998  0.4659825  0.46526521 0.46774721 0.46865507 0.4670353    0.46647046 0.4661633  0.46515267 0.46739333 0.46829748 0.46695525   0.46637392 0.46643638 0.46538408 0.46736604 0.46795546 0.46689723   0.46639042 0.46580706 0.46570842 0.46730488 0.46797343 0.46692994   0.46648205 0.46523362 0.46583365 0.46727405 0.46805885 0.46694349   0.46679161 0.46511849 0.46566786 0.46726443 0.46828195 0.46702229   0.4669886  0.46543566 0.46571878 0.46721896 0.46848619 0.46717234   0.46726817 0.46563528 0.46578883 0.46715861 0.46868739 0.46732198   0.46748845 0.46580326 0.4658701  0.46711037 0.4687329  0.4675856    0.46776417 0.46600852 0.46596323 0.46674899 0.46878989 0.46786311   0.46796931 0.46613814 0.46600606 0.46690859 0.46882395 0.46812498   0.46820165 0.4664107  0.46612857 0.46703234 0.46884182 0.46817077   0.46836855 0.46665978 0.46626806 0.46713428 0.46895935 0.46824681\n",
      " 0.62662996 0.63103378]                                              0.49845905 0.50046988]                                              0.48281726 0.48494193]                                              0.47741991 0.47796488]                                              0.4730236  0.47451829]                                              0.46991189 0.47254577]                                              0.46893793 0.46976152]                                              0.46804903 0.46899247]                                              0.46772412 0.46865964]                                              0.4681188  0.4691311 ]                                              0.46765988 0.46906917]                                              0.46794893 0.46854567]                                              0.46812971 0.46869366]                                              0.46855493 0.46897234]                                              0.46876273 0.46924712]                                              0.46887562 0.46947659]                                              0.46881515 0.46984204]                                              0.46857485 0.47004458]                                              0.46871138 0.47018102]                                              0.46888592 0.47015066]                                              0.46907241 0.47014628]                                              0.46940229 0.4701121 ]                                              0.46973125 0.47011512]                                              0.46977286 0.46998482]                                              0.46977492 0.46985245]                                              0.46981954 0.46963578]                                              0.46983234 0.46939552]                                              0.46989437 0.46918797]                                              0.46980546 0.46912829]                                              0.46983368 0.46906296]\n",
      "------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------  ------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\csata\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tabulate\\__init__.py:107: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  (len(row) >= 1 and row[0] == SEPARATING_LINE)\n",
      "C:\\Users\\csata\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tabulate\\__init__.py:108: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  or (len(row) >= 2 and row[1] == SEPARATING_LINE)\n"
     ]
    }
   ],
   "source": [
    "# SO for different variances this is the best model and best estimation\n",
    "min_index = np.unravel_index(np.argmin(Models_res, axis=None), Models_res.shape)\n",
    "print(min_index)\n",
    "print(Models_res[min_index])\n",
    "print(tabulate(Models_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR(epsilon=0.35789473684210527)\n",
      "0.4114847111925296\n"
     ]
    }
   ],
   "source": [
    "ind = (min_index[0]-1)*reg_numb+min_index[1]\n",
    "ind = min_index[0] * (eps_values * reg_numb) + min_index[1] * eps_values + min_index[2]\n",
    "Best_SVM_Model = Models[ind]\n",
    "print(Best_SVM_Model)\n",
    "print(Models_res[min_index[0]][min_index[1]][min_index[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4114847111925296\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred=Best_SVM_Model.predict(X_test_pca)\n",
    "mse_SVM = mean_squared_error( Y_test, y_pred)\n",
    "print(mse_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final_scaled = scaler.fit_transform(Final_x_test)\n",
    "X_Final = pca.transform(X_final_scaled)\n",
    "Final = np.delete(X_Final, Dropped, axis=1)\n",
    "\n",
    "\n",
    "Final_pred = Best_SVM_Model.predict(Final)\n",
    "WriteOutput(Final_pred,\"SVM_variance_threshold.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It gave a result of 0.41 in test set, and uploaded was 0.684."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaboost on modified data. It gave a great result before. now see if it's the same for reduced dims "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.716160877624121\n"
     ]
    }
   ],
   "source": [
    "# This had the best value according to grid search in firt assignment:\n",
    "# AdaBoostRegressor(learning_rate=0.31103448275862067, n_estimators=81, random_state=42) \n",
    "AdaBoost = AdaBoostRegressor(random_state=42, n_estimators=150, learning_rate = 0.1)\n",
    "\n",
    "\n",
    "AdaBoost.fit(X_train_pca, Y_train)\n",
    "\n",
    "y_pred = AdaBoost.predict(X_test_pca)\n",
    "mse_ada = mean_squared_error( Y_test, y_pred)\n",
    "rmse_ada = np.sqrt(mse_ada)\n",
    "print(rmse_ada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost with reduced features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5702304891182882\n"
     ]
    }
   ],
   "source": [
    "# This is XGBoost trying out with different values:\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "learn = XGBRegressor(n_estimators = 100, learning_rate = 0.1, max_depth = 20)\n",
    "learn.fit(X_train_pca, Y_train)\n",
    "\n",
    "y_pred = learn.predict(X_test_pca)\n",
    "mse_xb = mean_squared_error( Y_test, y_pred)\n",
    "print(mse_xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_down_time</th>\n",
       "      <th>std_down_time</th>\n",
       "      <th>max_down_time</th>\n",
       "      <th>sum_down_time</th>\n",
       "      <th>mean_up_time</th>\n",
       "      <th>std_up_time</th>\n",
       "      <th>max_up_time</th>\n",
       "      <th>sum_up_time</th>\n",
       "      <th>mean_length_activity</th>\n",
       "      <th>std_length_activity</th>\n",
       "      <th>...</th>\n",
       "      <th>change_12_std</th>\n",
       "      <th>change_13_count</th>\n",
       "      <th>change_13_mean</th>\n",
       "      <th>change_13_std</th>\n",
       "      <th>change_14_count</th>\n",
       "      <th>change_14_mean</th>\n",
       "      <th>change_14_std</th>\n",
       "      <th>count_action_time_grade2</th>\n",
       "      <th>max_cursor_grade2</th>\n",
       "      <th>max_word_count_grade2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.976000e+03</td>\n",
       "      <td>1.976000e+03</td>\n",
       "      <td>1.976000e+03</td>\n",
       "      <td>1.976000e+03</td>\n",
       "      <td>1.976000e+03</td>\n",
       "      <td>1.976000e+03</td>\n",
       "      <td>1.976000e+03</td>\n",
       "      <td>1.976000e+03</td>\n",
       "      <td>1976.000000</td>\n",
       "      <td>1976.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1976.00000</td>\n",
       "      <td>1976.000000</td>\n",
       "      <td>1976.000000</td>\n",
       "      <td>1976.000000</td>\n",
       "      <td>1976.000000</td>\n",
       "      <td>1976.000000</td>\n",
       "      <td>1976.000000</td>\n",
       "      <td>1976.000000</td>\n",
       "      <td>1976.000000</td>\n",
       "      <td>1976.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.688269e+05</td>\n",
       "      <td>4.189786e+05</td>\n",
       "      <td>1.769309e+06</td>\n",
       "      <td>2.683359e+09</td>\n",
       "      <td>7.689267e+05</td>\n",
       "      <td>4.189778e+05</td>\n",
       "      <td>1.769519e+06</td>\n",
       "      <td>2.683691e+09</td>\n",
       "      <td>6.139294</td>\n",
       "      <td>2.320964</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00521</td>\n",
       "      <td>0.466093</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.003841</td>\n",
       "      <td>0.334514</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.003490</td>\n",
       "      <td>6.278846</td>\n",
       "      <td>6.941802</td>\n",
       "      <td>7.343117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.951297e+05</td>\n",
       "      <td>1.098840e+05</td>\n",
       "      <td>2.919217e+05</td>\n",
       "      <td>1.746426e+09</td>\n",
       "      <td>2.951277e+05</td>\n",
       "      <td>1.098823e+05</td>\n",
       "      <td>2.919109e+05</td>\n",
       "      <td>1.746537e+09</td>\n",
       "      <td>0.709091</td>\n",
       "      <td>0.561703</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01111</td>\n",
       "      <td>2.149799</td>\n",
       "      <td>0.000937</td>\n",
       "      <td>0.012668</td>\n",
       "      <td>1.044791</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.009115</td>\n",
       "      <td>3.129068</td>\n",
       "      <td>3.207926</td>\n",
       "      <td>3.381960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.168521e+05</td>\n",
       "      <td>6.362678e+04</td>\n",
       "      <td>2.294520e+05</td>\n",
       "      <td>5.924404e+07</td>\n",
       "      <td>1.169387e+05</td>\n",
       "      <td>6.363192e+04</td>\n",
       "      <td>2.295480e+05</td>\n",
       "      <td>5.928792e+07</td>\n",
       "      <td>5.089888</td>\n",
       "      <td>0.688038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.186022e+05</td>\n",
       "      <td>3.615599e+05</td>\n",
       "      <td>1.754355e+06</td>\n",
       "      <td>1.529142e+09</td>\n",
       "      <td>6.187072e+05</td>\n",
       "      <td>3.615575e+05</td>\n",
       "      <td>1.754539e+06</td>\n",
       "      <td>1.529309e+09</td>\n",
       "      <td>5.693386</td>\n",
       "      <td>1.924295</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.781720e+05</td>\n",
       "      <td>4.348499e+05</td>\n",
       "      <td>1.792134e+06</td>\n",
       "      <td>2.297854e+09</td>\n",
       "      <td>7.782541e+05</td>\n",
       "      <td>4.348501e+05</td>\n",
       "      <td>1.792226e+06</td>\n",
       "      <td>2.298282e+09</td>\n",
       "      <td>5.969028</td>\n",
       "      <td>2.230252</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.135849e+05</td>\n",
       "      <td>4.878461e+05</td>\n",
       "      <td>1.807127e+06</td>\n",
       "      <td>3.440400e+09</td>\n",
       "      <td>9.136952e+05</td>\n",
       "      <td>4.878417e+05</td>\n",
       "      <td>1.807279e+06</td>\n",
       "      <td>3.440644e+09</td>\n",
       "      <td>6.354988</td>\n",
       "      <td>2.617704</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.897552e+06</td>\n",
       "      <td>1.545223e+06</td>\n",
       "      <td>8.313630e+06</td>\n",
       "      <td>2.424430e+10</td>\n",
       "      <td>7.897639e+06</td>\n",
       "      <td>1.545218e+06</td>\n",
       "      <td>8.313707e+06</td>\n",
       "      <td>2.424494e+10</td>\n",
       "      <td>10.831017</td>\n",
       "      <td>3.966525</td>\n",
       "      <td>...</td>\n",
       "      <td>0.06710</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>0.022941</td>\n",
       "      <td>0.149753</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.003294</td>\n",
       "      <td>0.057316</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>26.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 468 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean_down_time  std_down_time  max_down_time  sum_down_time  \\\n",
       "count    1.976000e+03   1.976000e+03   1.976000e+03   1.976000e+03   \n",
       "mean     7.688269e+05   4.189786e+05   1.769309e+06   2.683359e+09   \n",
       "std      2.951297e+05   1.098840e+05   2.919217e+05   1.746426e+09   \n",
       "min      1.168521e+05   6.362678e+04   2.294520e+05   5.924404e+07   \n",
       "25%      6.186022e+05   3.615599e+05   1.754355e+06   1.529142e+09   \n",
       "50%      7.781720e+05   4.348499e+05   1.792134e+06   2.297854e+09   \n",
       "75%      9.135849e+05   4.878461e+05   1.807127e+06   3.440400e+09   \n",
       "max      7.897552e+06   1.545223e+06   8.313630e+06   2.424430e+10   \n",
       "\n",
       "       mean_up_time   std_up_time   max_up_time   sum_up_time  \\\n",
       "count  1.976000e+03  1.976000e+03  1.976000e+03  1.976000e+03   \n",
       "mean   7.689267e+05  4.189778e+05  1.769519e+06  2.683691e+09   \n",
       "std    2.951277e+05  1.098823e+05  2.919109e+05  1.746537e+09   \n",
       "min    1.169387e+05  6.363192e+04  2.295480e+05  5.928792e+07   \n",
       "25%    6.187072e+05  3.615575e+05  1.754539e+06  1.529309e+09   \n",
       "50%    7.782541e+05  4.348501e+05  1.792226e+06  2.298282e+09   \n",
       "75%    9.136952e+05  4.878417e+05  1.807279e+06  3.440644e+09   \n",
       "max    7.897639e+06  1.545218e+06  8.313707e+06  2.424494e+10   \n",
       "\n",
       "       mean_length_activity  std_length_activity  ...  change_12_std  \\\n",
       "count           1976.000000          1976.000000  ...     1976.00000   \n",
       "mean               6.139294             2.320964  ...        0.00521   \n",
       "std                0.709091             0.561703  ...        0.01111   \n",
       "min                5.089888             0.688038  ...        0.00000   \n",
       "25%                5.693386             1.924295  ...        0.00000   \n",
       "50%                5.969028             2.230252  ...        0.00000   \n",
       "75%                6.354988             2.617704  ...        0.00000   \n",
       "max               10.831017             3.966525  ...        0.06710   \n",
       "\n",
       "       change_13_count  change_13_mean  change_13_std  change_14_count  \\\n",
       "count      1976.000000     1976.000000    1976.000000      1976.000000   \n",
       "mean          0.466093        0.000176       0.003841         0.334514   \n",
       "std           2.149799        0.000937       0.012668         1.044791   \n",
       "min           0.000000        0.000000       0.000000         0.000000   \n",
       "25%           0.000000        0.000000       0.000000         0.000000   \n",
       "50%           0.000000        0.000000       0.000000         0.000000   \n",
       "75%           0.000000        0.000000       0.000000         0.000000   \n",
       "max          44.000000        0.022941       0.149753        14.000000   \n",
       "\n",
       "       change_14_mean  change_14_std  count_action_time_grade2  \\\n",
       "count     1976.000000    1976.000000               1976.000000   \n",
       "mean         0.000095       0.003490                  6.278846   \n",
       "std          0.000311       0.009115                  3.129068   \n",
       "min          0.000000       0.000000                  1.000000   \n",
       "25%          0.000000       0.000000                  4.000000   \n",
       "50%          0.000000       0.000000                  6.000000   \n",
       "75%          0.000000       0.000000                  8.000000   \n",
       "max          0.003294       0.057316                 25.000000   \n",
       "\n",
       "       max_cursor_grade2  max_word_count_grade2  \n",
       "count        1976.000000            1976.000000  \n",
       "mean            6.941802               7.343117  \n",
       "std             3.207926               3.381960  \n",
       "min             1.000000               1.000000  \n",
       "25%             4.000000               5.000000  \n",
       "50%             6.000000               7.000000  \n",
       "75%             9.000000               9.000000  \n",
       "max            26.000000              26.000000  \n",
       "\n",
       "[8 rows x 468 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_down_time</th>\n",
       "      <th>std_down_time</th>\n",
       "      <th>max_down_time</th>\n",
       "      <th>sum_down_time</th>\n",
       "      <th>mean_up_time</th>\n",
       "      <th>std_up_time</th>\n",
       "      <th>max_up_time</th>\n",
       "      <th>sum_up_time</th>\n",
       "      <th>mean_length_activity</th>\n",
       "      <th>std_length_activity</th>\n",
       "      <th>...</th>\n",
       "      <th>change_12_std</th>\n",
       "      <th>change_13_count</th>\n",
       "      <th>change_13_mean</th>\n",
       "      <th>change_13_std</th>\n",
       "      <th>change_14_count</th>\n",
       "      <th>change_14_mean</th>\n",
       "      <th>change_14_std</th>\n",
       "      <th>count_action_time_grade2</th>\n",
       "      <th>max_cursor_grade2</th>\n",
       "      <th>max_word_count_grade2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_down_time</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.555150</td>\n",
       "      <td>0.656763</td>\n",
       "      <td>0.568564</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.555144</td>\n",
       "      <td>0.656693</td>\n",
       "      <td>0.568539</td>\n",
       "      <td>0.138443</td>\n",
       "      <td>0.110729</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020443</td>\n",
       "      <td>0.034924</td>\n",
       "      <td>0.027383</td>\n",
       "      <td>0.043841</td>\n",
       "      <td>0.039273</td>\n",
       "      <td>0.023359</td>\n",
       "      <td>0.017603</td>\n",
       "      <td>0.171527</td>\n",
       "      <td>0.076527</td>\n",
       "      <td>0.065505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_down_time</th>\n",
       "      <td>0.555150</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.481959</td>\n",
       "      <td>0.585829</td>\n",
       "      <td>0.555148</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.481907</td>\n",
       "      <td>0.585819</td>\n",
       "      <td>0.174909</td>\n",
       "      <td>0.138333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038470</td>\n",
       "      <td>0.048343</td>\n",
       "      <td>0.027077</td>\n",
       "      <td>0.041591</td>\n",
       "      <td>0.069972</td>\n",
       "      <td>0.023143</td>\n",
       "      <td>0.041216</td>\n",
       "      <td>0.355793</td>\n",
       "      <td>0.242255</td>\n",
       "      <td>0.237516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_down_time</th>\n",
       "      <td>0.656763</td>\n",
       "      <td>0.481959</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.407422</td>\n",
       "      <td>0.656766</td>\n",
       "      <td>0.481964</td>\n",
       "      <td>0.999987</td>\n",
       "      <td>0.407405</td>\n",
       "      <td>0.071394</td>\n",
       "      <td>0.072326</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008101</td>\n",
       "      <td>0.017053</td>\n",
       "      <td>0.010227</td>\n",
       "      <td>0.015692</td>\n",
       "      <td>0.027160</td>\n",
       "      <td>0.023189</td>\n",
       "      <td>0.017974</td>\n",
       "      <td>0.116025</td>\n",
       "      <td>0.056769</td>\n",
       "      <td>0.050014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum_down_time</th>\n",
       "      <td>0.568564</td>\n",
       "      <td>0.585829</td>\n",
       "      <td>0.407422</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.568554</td>\n",
       "      <td>0.585827</td>\n",
       "      <td>0.407496</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.325597</td>\n",
       "      <td>0.223625</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062847</td>\n",
       "      <td>-0.006384</td>\n",
       "      <td>-0.056131</td>\n",
       "      <td>-0.039973</td>\n",
       "      <td>0.151851</td>\n",
       "      <td>0.024989</td>\n",
       "      <td>0.078785</td>\n",
       "      <td>0.853661</td>\n",
       "      <td>0.639056</td>\n",
       "      <td>0.613803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_up_time</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.555148</td>\n",
       "      <td>0.656766</td>\n",
       "      <td>0.568554</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.555142</td>\n",
       "      <td>0.656695</td>\n",
       "      <td>0.568529</td>\n",
       "      <td>0.138423</td>\n",
       "      <td>0.110711</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020443</td>\n",
       "      <td>0.034927</td>\n",
       "      <td>0.027385</td>\n",
       "      <td>0.043844</td>\n",
       "      <td>0.039270</td>\n",
       "      <td>0.023359</td>\n",
       "      <td>0.017601</td>\n",
       "      <td>0.171515</td>\n",
       "      <td>0.076522</td>\n",
       "      <td>0.065500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>change_14_mean</th>\n",
       "      <td>0.023359</td>\n",
       "      <td>0.023143</td>\n",
       "      <td>0.023189</td>\n",
       "      <td>0.024989</td>\n",
       "      <td>0.023359</td>\n",
       "      <td>0.023147</td>\n",
       "      <td>0.023141</td>\n",
       "      <td>0.024989</td>\n",
       "      <td>0.043685</td>\n",
       "      <td>0.063351</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013714</td>\n",
       "      <td>0.014912</td>\n",
       "      <td>0.016979</td>\n",
       "      <td>0.045386</td>\n",
       "      <td>0.887876</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.936285</td>\n",
       "      <td>0.024584</td>\n",
       "      <td>0.009170</td>\n",
       "      <td>0.001588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>change_14_std</th>\n",
       "      <td>0.017603</td>\n",
       "      <td>0.041216</td>\n",
       "      <td>0.017974</td>\n",
       "      <td>0.078785</td>\n",
       "      <td>0.017601</td>\n",
       "      <td>0.041221</td>\n",
       "      <td>0.017947</td>\n",
       "      <td>0.078787</td>\n",
       "      <td>0.050400</td>\n",
       "      <td>0.065568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040411</td>\n",
       "      <td>0.016514</td>\n",
       "      <td>0.014993</td>\n",
       "      <td>0.042423</td>\n",
       "      <td>0.898406</td>\n",
       "      <td>0.936285</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.097645</td>\n",
       "      <td>0.074541</td>\n",
       "      <td>0.067195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count_action_time_grade2</th>\n",
       "      <td>0.171527</td>\n",
       "      <td>0.355793</td>\n",
       "      <td>0.116025</td>\n",
       "      <td>0.853661</td>\n",
       "      <td>0.171515</td>\n",
       "      <td>0.355797</td>\n",
       "      <td>0.116179</td>\n",
       "      <td>0.853682</td>\n",
       "      <td>0.310117</td>\n",
       "      <td>0.218403</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083511</td>\n",
       "      <td>-0.029040</td>\n",
       "      <td>-0.090417</td>\n",
       "      <td>-0.075983</td>\n",
       "      <td>0.178525</td>\n",
       "      <td>0.024584</td>\n",
       "      <td>0.097645</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.820041</td>\n",
       "      <td>0.797884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_cursor_grade2</th>\n",
       "      <td>0.076527</td>\n",
       "      <td>0.242255</td>\n",
       "      <td>0.056769</td>\n",
       "      <td>0.639056</td>\n",
       "      <td>0.076522</td>\n",
       "      <td>0.242260</td>\n",
       "      <td>0.057039</td>\n",
       "      <td>0.639083</td>\n",
       "      <td>-0.120791</td>\n",
       "      <td>-0.143867</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077527</td>\n",
       "      <td>-0.052304</td>\n",
       "      <td>-0.100270</td>\n",
       "      <td>-0.101339</td>\n",
       "      <td>0.133315</td>\n",
       "      <td>0.009170</td>\n",
       "      <td>0.074541</td>\n",
       "      <td>0.820041</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_word_count_grade2</th>\n",
       "      <td>0.065505</td>\n",
       "      <td>0.237516</td>\n",
       "      <td>0.050014</td>\n",
       "      <td>0.613803</td>\n",
       "      <td>0.065500</td>\n",
       "      <td>0.237520</td>\n",
       "      <td>0.050294</td>\n",
       "      <td>0.613830</td>\n",
       "      <td>-0.131499</td>\n",
       "      <td>-0.153477</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079145</td>\n",
       "      <td>-0.054460</td>\n",
       "      <td>-0.098898</td>\n",
       "      <td>-0.102209</td>\n",
       "      <td>0.123837</td>\n",
       "      <td>0.001588</td>\n",
       "      <td>0.067195</td>\n",
       "      <td>0.797884</td>\n",
       "      <td>0.972163</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>468 rows × 468 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          mean_down_time  std_down_time  max_down_time  \\\n",
       "mean_down_time                  1.000000       0.555150       0.656763   \n",
       "std_down_time                   0.555150       1.000000       0.481959   \n",
       "max_down_time                   0.656763       0.481959       1.000000   \n",
       "sum_down_time                   0.568564       0.585829       0.407422   \n",
       "mean_up_time                    1.000000       0.555148       0.656766   \n",
       "...                                  ...            ...            ...   \n",
       "change_14_mean                  0.023359       0.023143       0.023189   \n",
       "change_14_std                   0.017603       0.041216       0.017974   \n",
       "count_action_time_grade2        0.171527       0.355793       0.116025   \n",
       "max_cursor_grade2               0.076527       0.242255       0.056769   \n",
       "max_word_count_grade2           0.065505       0.237516       0.050014   \n",
       "\n",
       "                          sum_down_time  mean_up_time  std_up_time  \\\n",
       "mean_down_time                 0.568564      1.000000     0.555144   \n",
       "std_down_time                  0.585829      0.555148     1.000000   \n",
       "max_down_time                  0.407422      0.656766     0.481964   \n",
       "sum_down_time                  1.000000      0.568554     0.585827   \n",
       "mean_up_time                   0.568554      1.000000     0.555142   \n",
       "...                                 ...           ...          ...   \n",
       "change_14_mean                 0.024989      0.023359     0.023147   \n",
       "change_14_std                  0.078785      0.017601     0.041221   \n",
       "count_action_time_grade2       0.853661      0.171515     0.355797   \n",
       "max_cursor_grade2              0.639056      0.076522     0.242260   \n",
       "max_word_count_grade2          0.613803      0.065500     0.237520   \n",
       "\n",
       "                          max_up_time  sum_up_time  mean_length_activity  \\\n",
       "mean_down_time               0.656693     0.568539              0.138443   \n",
       "std_down_time                0.481907     0.585819              0.174909   \n",
       "max_down_time                0.999987     0.407405              0.071394   \n",
       "sum_down_time                0.407496     1.000000              0.325597   \n",
       "mean_up_time                 0.656695     0.568529              0.138423   \n",
       "...                               ...          ...                   ...   \n",
       "change_14_mean               0.023141     0.024989              0.043685   \n",
       "change_14_std                0.017947     0.078787              0.050400   \n",
       "count_action_time_grade2     0.116179     0.853682              0.310117   \n",
       "max_cursor_grade2            0.057039     0.639083             -0.120791   \n",
       "max_word_count_grade2        0.050294     0.613830             -0.131499   \n",
       "\n",
       "                          std_length_activity  ...  change_12_std  \\\n",
       "mean_down_time                       0.110729  ...      -0.020443   \n",
       "std_down_time                        0.138333  ...       0.038470   \n",
       "max_down_time                        0.072326  ...       0.008101   \n",
       "sum_down_time                        0.223625  ...       0.062847   \n",
       "mean_up_time                         0.110711  ...      -0.020443   \n",
       "...                                       ...  ...            ...   \n",
       "change_14_mean                       0.063351  ...       0.013714   \n",
       "change_14_std                        0.065568  ...       0.040411   \n",
       "count_action_time_grade2             0.218403  ...       0.083511   \n",
       "max_cursor_grade2                   -0.143867  ...       0.077527   \n",
       "max_word_count_grade2               -0.153477  ...       0.079145   \n",
       "\n",
       "                          change_13_count  change_13_mean  change_13_std  \\\n",
       "mean_down_time                   0.034924        0.027383       0.043841   \n",
       "std_down_time                    0.048343        0.027077       0.041591   \n",
       "max_down_time                    0.017053        0.010227       0.015692   \n",
       "sum_down_time                   -0.006384       -0.056131      -0.039973   \n",
       "mean_up_time                     0.034927        0.027385       0.043844   \n",
       "...                                   ...             ...            ...   \n",
       "change_14_mean                   0.014912        0.016979       0.045386   \n",
       "change_14_std                    0.016514        0.014993       0.042423   \n",
       "count_action_time_grade2        -0.029040       -0.090417      -0.075983   \n",
       "max_cursor_grade2               -0.052304       -0.100270      -0.101339   \n",
       "max_word_count_grade2           -0.054460       -0.098898      -0.102209   \n",
       "\n",
       "                          change_14_count  change_14_mean  change_14_std  \\\n",
       "mean_down_time                   0.039273        0.023359       0.017603   \n",
       "std_down_time                    0.069972        0.023143       0.041216   \n",
       "max_down_time                    0.027160        0.023189       0.017974   \n",
       "sum_down_time                    0.151851        0.024989       0.078785   \n",
       "mean_up_time                     0.039270        0.023359       0.017601   \n",
       "...                                   ...             ...            ...   \n",
       "change_14_mean                   0.887876        1.000000       0.936285   \n",
       "change_14_std                    0.898406        0.936285       1.000000   \n",
       "count_action_time_grade2         0.178525        0.024584       0.097645   \n",
       "max_cursor_grade2                0.133315        0.009170       0.074541   \n",
       "max_word_count_grade2            0.123837        0.001588       0.067195   \n",
       "\n",
       "                          count_action_time_grade2  max_cursor_grade2  \\\n",
       "mean_down_time                            0.171527           0.076527   \n",
       "std_down_time                             0.355793           0.242255   \n",
       "max_down_time                             0.116025           0.056769   \n",
       "sum_down_time                             0.853661           0.639056   \n",
       "mean_up_time                              0.171515           0.076522   \n",
       "...                                            ...                ...   \n",
       "change_14_mean                            0.024584           0.009170   \n",
       "change_14_std                             0.097645           0.074541   \n",
       "count_action_time_grade2                  1.000000           0.820041   \n",
       "max_cursor_grade2                         0.820041           1.000000   \n",
       "max_word_count_grade2                     0.797884           0.972163   \n",
       "\n",
       "                          max_word_count_grade2  \n",
       "mean_down_time                         0.065505  \n",
       "std_down_time                          0.237516  \n",
       "max_down_time                          0.050014  \n",
       "sum_down_time                          0.613803  \n",
       "mean_up_time                           0.065500  \n",
       "...                                         ...  \n",
       "change_14_mean                         0.001588  \n",
       "change_14_std                          0.067195  \n",
       "count_action_time_grade2               0.797884  \n",
       "max_cursor_grade2                      0.972163  \n",
       "max_word_count_grade2                  1.000000  \n",
       "\n",
       "[468 rows x 468 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\csata\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sweetviz as sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Summarizing dataframe]                      |          | [  0%]   00:00 -> (? left)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature: max_word_count_grade2               |██████████| [100%]   07:02 -> (00:00 left)                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report train_report.html was generated.\n"
     ]
    }
   ],
   "source": [
    "train_report = sv.analyze(X_train,pairwise_analysis='off')\n",
    "train_report.show_html('train_report.html', open_browser=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 0.9371527713737507\n"
     ]
    }
   ],
   "source": [
    "# Sklearn GradientBoostingRegressor method which is a bit different than xgboost, as it might be slower, but still I avhe implemented it\n",
    "gbt = GradientBoostingRegressor()\n",
    "gbt.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "sfm = SelectFromModel(gbt, threshold=0.1)\n",
    "sfm.fit(X_train, Y_train)\n",
    "X_train_selected = sfm.transform(X_train)\n",
    "X_test_selected = sfm.transform(X_test)\n",
    "\n",
    "gbt = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, random_state=42)\n",
    "gbt.fit(X_train_selected, Y_train)\n",
    "\n",
    "y_pred = gbt.predict(X_test_selected)\n",
    "\n",
    "rmse = mean_squared_error(Y_test, y_pred, squared=False)\n",
    "print(f'Root Mean Squared Error: {rmse}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_varSelected = mean_squared_error( Y_test, y_pred)\n",
    "mse_varSelected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=20, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=200, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=20, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=200, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=20, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=200, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb_model = XGBRegressor(n_estimators = 200, learning_rate = 0.1, max_depth = 20)\n",
    "\n",
    "xb_model.fit(X_train_pca, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5700900788535235"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = xb_model.predict(X_test_pca)\n",
    "mse = mean_squared_error(Y_test, y_pred)\n",
    "mse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
